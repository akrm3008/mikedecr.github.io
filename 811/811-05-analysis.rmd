---
title: "Lecture 3: Analysis"
author: "Michael DeCrescenzo"
description: "Statistical analysis, model output, and workflow integration"
date: "2018-03-01"
slug: "811-analysis"
categories: ["R", "ps811", "Teaching"]
tags: []

draft: true
---



---

# Sarah

- why
  + helpful to see what the data look like
  + tell persuasive stories
  + important that you know relevant features of the data
  + shape of the data indicate appropriate modeling
    * *long right tails, logging*
- Understanding data
  + *know thy data*
  + unit of analysis
  + number of observations
  + where is there variation
  + units of measurement
  + values within variable
  + missing data
  + relationships among variables
- summary statistics
  + `summary, cor, quantile`
  + `table()`
  + `group_by() %>% summarize()`
  + `group.CI()` or whatever it's called
- simple base plots
  + scatterplots
  + histograms
  + kernel density estimation
- *transition to ggplot*
  + *data, axis aesthetics, geoms, scales, theme options*
  + *ggplot() and qplot() functions. qplot() is just confusing IMO* 
  + *justify gglot*
    * *more with less code*
      - *grouping*
      - *faceting*
      - *legends*
    * *prevents you from doing stupid shit*
      - multiple y axes
      - bad aesthetic choices (multiple DVs at once)
    * *saving is easier*
    * *they look better*
    * *easy to customize your own cute theme (help guides online but don't include a link [don't want them to get sidetracked] but don't overdo it)*
- *once in GGplot*
  + scatterplots (two continuous)
  + dot plots (factor x continuous) *replace bar plots*
  + line graphs (two continuous where order matters, need to be sorted)
  + don't use pie charts
    * *anything in a pie chart should be a bar graph*
  + bar graphs
    * *only things in a bar graphs should be counts and proportions, IMO*
    * *stacks of things that have a zero lower bound*
    * *don't use them for 1-5 scales*
    * *IMO don't use them for treatment effects. This makes no sense*
    * *they also make you use error bars on top of columns for CIs, which looks fugly IMO*
  + *coefficient plots (wait on this?)*
  + *acf pacf* with some package
- aesthetic choices
  + contrasting colors may be bad for colorblindness
  + gradients of the same color may be good for computer screens, but maybe not for print or shitty projectors
  + does the journal accept color? might want to think about ways around color
  + resources
    * http://mkweb.bcgsc.ca/colorblind/
    * http://www.somersault1824.com/tips-for-designing-scientific-figures-for-color-blind-readers/
    * http://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3
    * http://ggplot2.tidyverse.org/reference/scale_brewer.html
    * `viridis`! *but don't overdo it*
- saving
  + base
    * `pdf("file.pdf", width =, height =); par(whatever); plot(whatever); dev.off()`
  + ggplot
    * `ggplot(whatever); ggsave(whatever, width =, height =)`
- Complex data analysis
  + *note to self: learn about arm, rms*
    * *if you don't get to this, just say that you don't do regressions that much, and you don't do frequentist regression when you do, LOL*
  + regression functions
    * transform within function call (logs, interaction, factor dummies)
  + glms
    * ols: y = (XB)
    * glm: link(y) = XB; y = inv_link(XB)
      - logit(p(y)) = XB. invlogit(XB) = p(y)
  + interpreting model output
  + generating model output
    * table packages
      - list: https://stackoverflow.com/questions/5465314/tools-for-making-latex-tables-in-r
      - stargazer: https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf
      - apsrtable: https://cran.r-project.org/web/packages/apsrtable/apsrtable.pdf
      - *I like xtable and texreg*
      - there are pros and cons. Some are better for regression, summary statistics, just other tables you might make (xtable)
    * `arm::coefplot`
    * `coefplot` package
    * `coefplot2`
    * I'd say: coefplots by hand using `broom` and `ggplot2`. Easy for jamming multiple models together
      - http://varianceexplained.org/r/broom-intro/
      - https://arxiv.org/pdf/1412.3565.pdf
    * *I've seen people do coefplots using base graphics and it's like half the entire file. It's fugly*
  + marginal effects
    * effects
    * zelig
    * *sjplot*
  + post-estimation
    * residuals 
    * (most models: normal on the index. OLS index = link)
    * *Log Likelihood* (coef.test? where are there model things for GLMs?)
    * *information criteria: AIC, WAIC*
      - meant to estimate out-of-sample performance
      - see McElreath videos
      - since every fit is a combination of true + noise (even assuming perfect model)
    * don't BIC
      - neither Bayesian nor IC


Improving on Sarah

- how tidy data becomes good for ggplot
  + stacking factors for grouping aesthetics
- nail grouping as a benefit
  + if you have multiple countries, multiple states, multiple datasets
  + you *can* gather multiple DVs, but then you can't get different axis labels, so do this only for private use OR with conceptually related variables for public use
    * *use var-ecm graphic as a bad example*
- What to do about model graphics
  + a simple regression example
  + use broom for coefficient plot
  + sjplot for a simple marginal effects?
  + broom: learn how `glance` works
  + `predict(new.data)`
  + effects?
  + margins?
  + arm?
  + rms?
  + *and then save complex predictions for modeling day?*
- Other graphics tips
  + confidence interval things:
    * steep change look like narrow confidence intervals
    * confidence intervals are *vertical* for a given x value (find tumblr example)
  + log(0)
    * are they counts? use a count model instead of a log
    * are they unequal probability? use an ordinal model
- stata vs R
  + if you have a simple OLS or GLM and want to do a simple prediction or simple MFX, Stata does this pretty well
  + R is way better for visualizing multiple models together (tidy data, broom, ggplot)




# more Sarah

- "advanced topics"
  + looping
  + apply
  + nesting and mapping
  + lists
  + type coercion
- things I plan to do already
  + pipes
  + dplyr
  + tidyverse
  + forcats
  + haven
  + readr
  + tidyr
  + stringr
- what is the tidy time series thing? hms?
- regex
  + just explain what it is. Not worth learning and not worth teaching right now
- text analysis: 
  + tm
  + tidytext
- time series stuff
  + lubridate
  + what other time series packages are out there
  + Stata is maybe better?
- R as front end
  + rcpp, sql, rpython (rselenium)
- learning new packages
- survey analysis
  + Sarah has a good story example!
  + Stata might be better
- Bayesian analysis: 
  + bugs, jags, stan
    * Jags probably easiest for custom models
      - learn JAGS from Gelman and Hill (who use BUGS but it's very similar)
      - Jackman Bayes textbook
    * Stan definitely the fastest but learn r-like (requires a C++ like file)
      - Gelman et al BDA
  + other packages for quick and dirty Bayes things
    * `brms`
    * `rethinking`
  + *decide how much you want to preach*


My advanced things:

- hierarchical models
  + figure out how badly you want this...
  + you like these but don't actually do them all that much
- function writing
- bayesian example
- apply and lapply
- nest and map
  + hierarchical: mrp?




# Introduction

- gelman quote about dumbest possible thing



# Nonstatistical approaches


## tabulating

## aggregating



# Univariate statistics


## means and CIs

## comparing CIs across groups

## simple means tests, chisq tests




# Regression

## lm

rescaling

## glm

Intuition of the GLM

- $E[Y] = g^{-1}(X\beta)$
- $Y = E[Y] + \epsilon$
- or $Y \sim \mathcal{D}(\theta)$


## lme4


## bayesian models



# Regression output


## regression tables

`xtable`, `stargazer`, `texreg`


## regression coef plots

don't use `coeftest`


`broom`,


## regression predictions

maybe avoid `effects`

use `sjplot`, `margins` for R


## marginal effects




## Time Series

- Time series is just hard and you're going to spend a lot of time beating your head against a wall. There is nothing I can do to fix this. But here are some tools that you should check out to help you.
- Fixing data: 
  + stata has `tsset` and you feed it a date, maybe a group
  + dplyr: `group_by(whatever)`, `arrange(date)`
  + lags and leads just work on rows, so it won't *know* to grab the previous date
- `zoo`? Creates an index variable
- `xts` requires the index to be a date
- lubridate for date/time modification
- RcppRoll for rolling averages
- tidyquant has period-level apply functions (apply something weekly, for example)





# User-defined functions

This section is a little complicated, so it's okay if it doesn't make perfect sense at first.

In R, it is easy to write your own functions. Here is an example of a function that calculates a mean. 

```{r}
my_mean <- function(z) {

  sum_z <- sum(z)
  n <- length(z)
  the_mean <- sum_z / n
  
  return(the_mean)

}
```

Here's how this works. 

First, we pick a name for the function. Here, we chose `my_mean`. If you don't know if there are any existing functions with the same name, you can search for it in R's help files (`?my_mean` and `??my_mean`).

Second, we decide which *arguments* this function should have. An argument is information that gets *passed* to the function. That's what the `function(z)` is doing---the function takes one argument called `z`. We could have written `function(banana)` if we wanted, and then manipulated the `banana` object inside the function. The `z` (or `banana`) object name is just a stand-in for information that we are going to manipulate inside the function definition. 


Third, the function needs a *definition* inside curly braces, meaning, what the function does. Technically, we *assign* a definition to the object called `my_mean`. (Yes, even functions are objects.) If we passed some variable to `my_mean()`, the function calculates the sum, finds the length, and then divides the sum by the length. Finally, the function *returns* the calculated value. In all, when we pass in some `z` data, we get out some value that is the result of `the_mean`.


For example, let's make some data. 

```{r}
(some_data <- seq(0, 10, 2))
```

(Notice that the `seq(a, b, c)` function creates a sequence of values from `a` to `b` with a skip interval of `c`.) And then we will find the mean of this variable using our new mean function.

```{r}
my_mean(some_data)
```

There is one potentially-confusing point I need to make about custom functions. The variables that are manipulated inside the function definition are called *local variables*. This means they only exist in the world of that function. Although the function manipulates variables called `z`, `sum_z`, and so on, those variables only exist to that function and are not accessible to you in the rest of R's memory. They exist only to make the function work. To demonstrate the point, print out all of the objects in R's memory using the `ls()` function.

```{r}
ls()
```

You will notice that none of the objects created by `my_mean()` are there. This is by design, because you don't want any function you create to get confused about whether the object you want to manipulate is local or global. 



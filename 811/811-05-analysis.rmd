---
title: "Lecture 3: Analysis"
description: "Statistical analysis, model output, and workflow integration"
author: "Michael DeCrescenzo"
date: '2018-02-23'
slug: "811-analysis"
categories: ["R", "ps811", "Teaching"]
tags: []
---




# Schedule

Read this before our final lecture, after the [graphics lesson](811/811-graphics).




# How to follow along

A script file walking through some of these commands is available [here](https://uwmadison.box.com/s/r224nk4rjll638dhndigeksvhwjuaq3q).

# Objectives

In this lesson, we will introduce how to do statistical analysis using R. Topics that you should cover to prepare for the take-home exercise include...

- Means, confidence intervals, and simple significance tests
- Estimating regression models
- Generating model output
- Model diagnostics and fit statistics
- Post-estimation graphics (model predictions)

This page also contains some content on more advanced topics, but these won't be necessary for the take-home exercise. These include...

- intermediate R tools and routines
  + lists
  + `apply()` functions
  + mapping functions to nested data frames
  + custom functions
  + type coercion
- a reference list of tools for advanced analysis
  + time series
  + panel models
  + multilevel models
  + Bayesian models
  + etc.

Since we should be getting used to R, I will sprinkle some more interesting data manipulation tricks into the analysis. Pay careful attention, as some of these tricks may come in handy in the future! As always, I recommend you run pipe chains chunk by chunk so you can see how each function in the chain contributes to the final result. 


# Data and packages


Load packages. 

```{r, eval = FALSE}
library("Rmisc")
library("magrittr")
library("tidyverse")
library("ggplot2")
library("broom")
```


```{r, include = FALSE}
library("Rmisc")
library("magrittr")
library("tidyverse")
library("ggplot2")
library("broom")
```


Set the default `ggplot` theme.

```{r}
theme_set(theme_bw())
```



Set your directory and read data from that we saved at the end of the previous lesson.

```{r, eval = FALSE}
setwd("~/path/to/wherever")
anes <- readRDS("data/anes-modified-2.RDS") %>% print()
```

```{r, include = FALSE, cache = TRUE}
anes <- readRDS(here::here("static/data/anes-modified-2.RDS")) %>% print()
```



# Non-statistical analysis

Previous lessons covered two major types of non-statistical analysis. We saw how to create some simple tables of variables in your data (using either the `table()` function or the `count()` function). We also saw how to create graphics, which are a major arena of non-statistical analysis. 


# Estimates and confidence intervals

When we generate estimates from data, we are usually interested in the point estimate and the uncertainty in that estimate. 

The `Rmisc` package has `CI` and `group.CI` functions for estimating means with confidence intervals. The `group.CI()` function is better than `CI()` for several reasons, so we'll use for subgroup estimates and ungrouped estimates.

For ungrouped estimates, we use the following syntax.

```{r}
# CIs for no groups
#   place a `1` where you'd otherwise put a group variable
group.CI(libcon_self ~ 1, data = anes)
```

Remember that every confidence interval has associated assumptions. This interval assumes that the sampling distribution of the mean is normally distributed. This is often a fine assumption, but the fact that we have only seven valid values makes this variable slightly problematic. (You would probably not get pushback for doing this though, because most people don't think about these assumptions).

You can add a grouping variable like so. We'll estimate the mean ideological self-placement within each party ID on the 7-point partisanship index.

```{r}
group.CI(libcon_self ~ pid7, data = anes)
```

The `group.CI()` function returns a data frame, so you could plot this pretty easily. We'll add a dividing line at "moderate."

```{r}
# calculate mean self-placement, by party ID
# since 2000 only

mean_ideo <- anes %>%
  filter(cycle >= 2000) %>% 
  group.CI(libcon_self ~ pid7, data = .) %>%
  print()

# plot self-placement over party ID
# modifying axis scales
# confidence intervals are there, just small

ggplot(mean_ideo, aes(x = as.factor(pid7), y = libcon_self.mean)) + 
  geom_hline(yintercept = 4, color = "gray50") +
  geom_pointrange(aes(ymin = libcon_self.lower, ymax = libcon_self.upper)) +
  labs(y = "Ideological Self-Placement",
       x = "Party ID") +
  scale_x_discrete(labels = c("Strong\nDem", "Weak\nDem", "Lean\nDem", "Independent", "Lean\nRep", "Weak\nRep", "Strong\nRep")) +
  scale_y_continuous(breaks = 1:7,
                     labels = c("Very Lib", "Lib", "Slight Lib", 
                                "Moderate", 
                                "Slight Con", "Con", "Very Con")) +
  coord_cartesian(ylim = c(1, 7)) +
  theme(panel.grid.minor = element_blank())
```

Be warned. Because the `group.CI()` function returns a data frame and not just a single value, it will make `dplyr::summarize()` upset if you try to group a data frame and estimate that way. We can show an advanced way of dealing with this later (nesting and mapping). 

Let's do one more example where we try to detect some evidence of ideological polarization/sorting over time. We'll track how the mean ideology among Democrats and Republicans changes over time. We use the `as_data_frame()` function to convert the table to a tibble (for nicer printing).

```{r}
# collapse party into Ds and Rs, else NA
# Find mean self-placement in each party, in each cycle
# convert to data_frame for prettier printing

sorting <- anes %>%
  mutate(party = case_when(pid7 %in% 1:3 ~ "Democrat",
                           pid7 %in% 5:7 ~ "Republicans")) %>%
  group.CI(libcon_self ~ party + cycle, data = .) %>%
  as_data_frame() %>%
  print()

# plot ideological self-placement
# Democrats and Republicans across time

ggplot(sorting, aes(x = cycle, y = libcon_self.mean, color = party)) +
  geom_line(show.legend = FALSE) +
  geom_point(show.legend = FALSE) +
  geom_ribbon(aes(ymin = libcon_self.lower, ymax = libcon_self.upper, 
                  fill = party),
              alpha = 0.3,
              color = NA,
              show.legend = FALSE) +
  coord_cartesian(ylim = c(1, 7)) +
  annotate("text", x = 2000, y = 2.5, label = "Democrats") +
  annotate("text", x = 2000, y = 6.25, label = "Republicans") +
  scale_y_continuous(breaks = 1:7,
                     labels = c("Very\nLiberal", "Liberal", "Slightly\nLiberal", 
                                "Moderate", 
                                "Slightly\nConservative", "Conservative", "Very\nConservative")) +
  scale_x_continuous(breaks = seq(1972, 2012, 8)) +
  scale_color_manual(values = c("dodgerblue", "maroon")) +
  scale_fill_manual(values = c("dodgerblue", "maroon")) +
  labs(x = "Election Cycle",
       y = "Mean Ideological Self-Placement",
       color = NULL, fill = NULL) +
  theme(panel.grid.minor = element_blank())
```


## Proportions

For proportions, one could use `prop.test()` for the normal approximation method (which most people learn in school), or `binom.test()` for "exact" Clopper-Pearson intervals, which have better boundary assumptions and small-sample properties (they are estimated using quantiles of the beta distribution), but they can be conservative (a little wide, more than 95% coverage in some cases). They both work in R basically the same way though.

```{r}
# find the number of democratic voters in 2012, say. Sum the TRUEs
dem_voters <- anes %>%
  filter(cycle == 2012) %$% 
  sum(vote == "Democratic Candidate", na.rm = TRUE) %>%
  print()

# find the num. of major party voters in 2012
twoparty_voters <- anes %>%
  filter(cycle == 2012) %$% 
  sum(vote %in% c("Democratic Candidate", "Republican Candidate"), 
      na.rm = TRUE) %>%
  print()

# estimate demvoters / majorparty voters, with CI
results <- anes %$% 
  prop.test(dem_voters, twoparty_voters)

# view results
results

# this is a complex object. See what's inside of it
attributes(results)

# let's grab the point estimate. 
# We can use $ to go "inside" this object
results$estimate

# grab the confidence interval in the same way
# it is a two-element vector (with some metadata)
results$conf.int
```

As you can see, these old hypothesis testing functions produce weird objects as output, making them feel ancient and complex. It gets tougher to organize mentally if you need to estimate proportions for multiple groups. I'll show you how, but it's a little complicated.

```{r}
# Objective
# a numerator and a denominator in every row
# group the data and apply the test function to each row

# groups = party ID
# find num of dem voters (numerator) and major party voters (denominator)

# In this chain, we take the sum of a logical variable
#   in arithmetic, logical variables are like dummy variables
#   (TRUE is treated as a 1, FALSE as a 0)

grp_raw <- anes %>%
  filter(cycle == 2012) %>%
  filter(!is.na(pid7)) %>% 
  group_by(pid7) %>%
  summarize(dem_voters = 
              sum(vote == "Democratic Candidate", na.rm = TRUE),
            twoparty_voters = 
              sum(vote %in% c("Democratic Candidate", "Republican Candidate"),
                  na.rm = TRUE)) %>%
  print()

# think of binom.test() as being run separately for each row (group)
# since each row is a group in this case
# separately save mean, lower and upper bounds
grp_prop <- grp_raw %>%
  group_by(pid7) %>%  
  mutate(prop = binom.test(dem_voters, twoparty_voters)$estimate,
         lower = binom.test(dem_voters, twoparty_voters)$conf.int[1],
         upper = binom.test(dem_voters, twoparty_voters)$conf.int[2]) %>%
  print()


# plot estimates with CIs as pointranges (points plus error bars)

ggplot(grp_prop, aes(x = as.factor(pid7), y = prop)) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  labs(x = "Party ID",
       y = "Democratic share of two-party vote",
       caption = "ANES 2012 data")
```

Some people have developed tools to make it easier to work with these old functions. The `broom` package is amazing one. Let's use the `broom::tidy` function to clean up the output from the `prop.test()` function from above.

```{r}
# results object from before
results 
# tidy results
tidy(results)
```

The `tidy()` function returns a tidy frame with columns for estimates, test statistics, $p$-values, confidence interval bounds, and so on. You could run `tidy()` on lots of different proportions tests, stack them into one data frame, and then plot the results in cool ways. We'll do something like that later when we cover regression models.

For formal hypothesis testing of means, the `t.test()` function works a lot like `prop.test()` and `binom.test()`. I won't beat this lesson to death though.



# Regression

And now, the good stuff.

R has functions for linear and generalized linear models. They work pretty similarly, with some important exceptions. First, we'll review regression in general.

A linear model estimates a "predicted value of $y$" (that is, the *mean of $y$, conditional on $x$*) assuming that the observed data are the conditional mean plus a (normally distributed) residual. We could write that a few ways, but let's start with a familiar way from PS-813.

$\begin{align} y_{i} &= \alpha + \beta x_{i} + \varepsilon_{i}  \\[6pt] \varepsilon_{i} &\sim \mathrm{Normal} \left( 0, \, \sigma \right) \end{align}$

Each predicted value of the dependent variable ($\hat{y}_{i}$) is a regression on a set of $x$ variables and coefficients $\beta$ and a constant $\alpha$, and residuals are normally distributed with mean of $0$ and some estimated standard deviation $\sigma$.

Here's how we estimate regression equations in R, generically, using the `lm()` function.

```{r, eval = FALSE}
# this is an example with multiple x variables
# to show that you use the `+` to specify the additive equation form
model_results <- lm(y ~ x1 + x2 + x3, data = dataset_name)
```

The syntax can read that `y` is a function of `x1`, `x2`, and so on. We also must specify the data set where these data come from with `data =`.

After estimating, we usually look at detailed results using the `summary()` function.

```{r, eval = FALSE}
summary(model_results)
```

Here's a real example, predicting relative thermometer ratings ($y$) using ideological self-placement ($x$), with data from the 2000 election only (note the use of `filter()` in the `data = ` argument). It seems reasonable that individuals who are more conservative are more likely to feel warmer toward the Republican candidate than they are toward the Democratic candidate.

```{r}
# relative feeling thermometer as a function of ideology

therm_mod <- lm(reltherm_cand ~ libcon_self, 
                data = filter(anes, cycle == 2000))

# regression summary

summary(therm_mod)
```

The `summary` function shows us the estimated coefficients (for the intercept and predictor), standard errors, test statistics, p-values, and significance levels. We also get some information about the F test and explained variance.



## Regression tricks 

Let's do a multiple regression example. How about we just do this ideological scale as a series of dummy variables instead of as a continuous predictor. We can coerce any variable to series of dummy variables by inputting the variable as a factor in the regression equation.

```{r}
# "dummy mod": ideology as a set of dummies using as.factor()

dummy_mod <- lm(reltherm_cand ~ as.factor(libcon_self), 
                data = filter(anes, cycle == 2000))

# each effect is an offset relative to the constant

summary(dummy_mod)
```

The `as.factor()` coerced the `libcon_self` variable, which is originally numeric (integers), to be treated as a factor variable. Whenever `lm()` encounters a factor as an independent variable, R interprets the factor as a set of dummy variables automatically, treating the "first" level as the omitted category. It's important to remember that when we have dummy variables, each coefficient should be interpret as an *offset* relative to the intercept. Ordinarily you want to take great care to specify which category should be omitted, because significance testing for the dummy variables will be relative to that baseline category.

If we want, we could rewrite this model by suppressing the intercept entirely with `-1`. In that case, we would have no omitted category, so each estimated coefficient would essentially represent the mean for each group. 

```{r}
# all-intercepts model (no omitted category => no constant)

int_mod <- lm(reltherm_cand ~ -1 + as.factor(libcon_self), 
                data = filter(anes, cycle == 2000))

# each estimate is a group mean

summary(int_mod)
```

This model makes a great deal of sense just looking at it. Each intercept is the estimated mean response for each level of party ID. When respondents are more liberal, they like the Democratic candidate more than the Republican candidate, and vice versa for more conservative respondents. Watch out for significance testing though: right now, p-values are only testing against the null hypothesis that each coefficient is different from zero. You'd have to do some extra work to determine whether one intercept is statistically different from another.

We could, if we want, estimate an intercept-only model (no predictors) of the following form:

$\begin{align} y_{i} &= \alpha + \varepsilon_{i} \end{align}$ 

by writing the formula as `y ~ 1`.  This is also how we used the `group.CI()` to estimate means without any groups. 

```{r}
# intercept-only model

alpha_mod <- lm(reltherm_cand ~ 1, 
                data = filter(anes, cycle == 2000))

# this makes sense in the math, if you think about it
# the right hand data is a "1"
# y = alpha*1 is equivalent to y = alpha

# (if you've seen regression in matrix before, 
#  this is why the first column in the X matrix is a set of 1s)

summary(alpha_mod)
```



# Model output: tables

Like Stata, R can produce regression tables for you to insert into your writing. You should use these functions.

In fact, my advice is that you should never, never, ***NEVER*** write up a regression table by hand. Not in $\mathrm{\LaTeX}$, not in Word, not ever. 

- You are likely to make some kind of transcription error by hand-typing numbers into the table
- If your analysis ever changes, even slightly, you need to modify your table or create a brand-new one
- Spend your time researching and doing stats, not punching tables into the computer

R provides several packages for formatting the output of an analysis into tables. This output could be formatted as $\mathrm{\LaTeX}$ code, as HTML code, as plain text, and so on. We'll play with some table packages using the models we just estimated.

The `stargazer` package is solid and highly customizable. Its tables look good, but I find that it requires a lot of customization to make the tables look *great*.

```{r, eval = FALSE}
library("stargazer")
# latex by default
stargazer(therm_mod, dummy_mod, int_mod)
# as plain text
stargazer(therm_mod, dummy_mod, int_mod, type = "text")
```

I think I like `texreg` a little better. I'm fairly sure it supports more model types than `stargazer`, and its defaults are a bit more sensible than `stargazer`'s are, at least for my uses (your uses may be different).

```{r, eval = FALSE}
library("texreg")
texreg(list(therm_mod, dummy_mod, int_mod))
```

I love `xtable` for *non-regression tables* such as marginals and summary statistics. The tables are clean, slick, and you can get them to do what you want. (See the tables at the end of [this document](https://elections.wisc.edu/news/Voter-ID-Study/Voter-ID-Study-Supporting-Info.pdf), for example.) However, `xtable` is weirder for regression, and it's strange in general because it often requires you to place more arguments into the `print()` function, which is counter-intuitive. But these tables look great when you can make it work.

```{r, eval = FALSE}
library("xtable")
print(xtable(therm_mod))
```


These aren't the only tools for making tables in R. Check [here](https://stackoverflow.com/questions/5465314/tools-for-making-latex-tables-in-r) for more info.

My general practice is to rely *only* on code to make tables. This ensures that you don't make any transcription errors. It also ensures that when you export these tables to an external file (next section), any update to the model in R is automatically updated in your paper (if using $\mathrm{\LaTeX}$). 

Here's how I advise you learn about tables:

- Experiment with the packages to figure out which package you like best
- Figure out which modifications to the default tables you tend to use the most
- Save these modifications somewhere (in a file, or as a keyboard shortcut), to save yourself some time whenever you write regression tables into a paper



# Exporting regression tables


This is not that difficult, but it requires some thinking about your project directory.

Whenever you have some project, your directory should contain separate folders for `R/`, `data/`, and your writing (which I call `tex/` or `paper/`). Let's assume that we're writing a $\mathrm{\LaTeX}$ document inside a folder called `tex/`, and we want to save a regression table from R. We should also assume that our directory in R is set to the *project root*, i.e. the top of the project folder (for example, `"users/michaeldecrescenzo/box sync/research/thesis"`).

We can use R to create a subfolder inside of the `tex/` folder dedicated to tables, if we don't already have one.

```{r, eval = FALSE}
# makes a "tex" folder
dir.create("tex")


# make a tables/ folder inside of tex/
# requires a tex/ folder to exist already

dir.create("tex/tables")
```

We can then save model output as a `.tex` file in this folder location. Table-creating functions allow to you specify where you want the table to save. Specify the name of the `.tex` file you want to save. (You can also save tables as plain text and HTML files, depending on the package, but you get the most direct utility by learning to use $\mathrm{\LaTeX}$ and saving tables as $\mathrm{\TeX}$ files).

```{r, eval = FALSE}
# save texreg table (with caption) as a .tex file, in specified location

texreg(list(therm_mod, dummy_mod, int_mod),
       caption = "Estimated regression results, various specifications",
       file = "tex/tables/reg-table.tex")
```

And then in your `research-paper.tex` file, you can include a code to insert code from another `.tex` file somewhere else.

```
... blah blah blah. See the regression table for results.

\input{tables/reg-table}

The regression table shows that...
```


# Model output: graphics

New tools in R make it very easy to produce graphical model output.

We'll talk about the `broom` package, which we've already introduced somewhat. We can turn model output into a tidy data frame using `tidy()`.

```{r}

# Tidy the all-intercepts model
# separate columns for variable names, coefficients, std err, p-val...

tidy(int_mod)
```

Combine tidy model data frames using `bind_rows()`, which binds data frame rows together. We do this while also adding a variable for the model name (using `mutate()`). 

```{r}
mods <- bind_rows(mutate(tidy(therm_mod, conf.int = TRUE), 
                           model = "Continuous Predictor"), 
                  mutate(tidy(dummy_mod, conf.int = TRUE), 
                           model = "Constant and Dummies"), 
                  mutate(tidy(int_mod, conf.int = TRUE), 
                           model = "All Intercepts")) %>%
  as_data_frame() %>%
  print() 
```

We can then plot coefficients straight away. Use the variable name as the `x` and the coefficient as the `y`

```{r}
# we flip the x and y coordinates 
#   to imitate the typical look of a coefficient plot
# Notice how not every model has the same variable names...

ggplot(mods, aes(x = term, y = estimate)) +
  geom_hline(yintercept = 0, color = "gray50") +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high,
                      color = model),
                  position = position_dodge(width = -1)) +
  scale_color_brewer(palette = "Set2") +
  coord_flip() +
  labs(x = NULL, y = "Estimated Coefficient", color = "Specification")
```

So easy! You could then save this plot...

```{r, eval = FALSE}
# creates a graphics folder, if one doesn't already exist
dir.create("tex/graphics")

# save the plot in the graphics folder, setting height and width
# saves as PDF
ggsave("tex/graphics/coefplot.pdf", height = 5, width = 5)
```

Some other tools create these sorts of plots for you. Some folks enjoy using [`sjPlot`](http://www.strengejacke.de/sjPlot/sjp.lm/), but I have never bothered to use it. (I ordinarily don't like packages to make plots for me. I prefer to create *data* and decide for myself how they should be plotted. This is my personal preference, though---you do you.)




## Related: saving other quantities (from R to $\mathrm{\LaTeX}$)

Just like with tables, you can save many other quantities to `tex` files. This way, the quantities in your paper reflect quantities in the analysis *perfectly*.

For example, let's calculate the mean GOP candidate thermometer in (say) 2012, and save it.

```{r, eval = FALSE}
# create a subdirectory for referenced values from R
# I call this folder location "refs" but you can call it whatever you want
dir.create("tex/refs")

# in the 2012 cycle, 
# what is the mean thermometer rating of GOP candidate?
# round it to the nearest whole number
# print the value to see what it looks like
# save the value in a .tex file for later use

anes %>% 
  filter(cycle == 2012) %$% # <--- note the pipe!
  mean(therm_gopcand, na.rm = TRUE) %>%
  round() %>%
  print() %>% 
  write("tex/refs/mean-gop-therm-2012.tex") 
```


Then, in your `.tex` file, import this quantity directly from file...

```
...the mean rating for Mitt Romney was $\input{refs/mean-gop-therm}$
```

...which would automatically grab the contents of that saved `tex` file and place it into your paper when you compile the `tex` document! This practice cuts down human error, saves time (you no longer have to update everything in your `.tex` file by hand every time you slightly change an analysis), and enhances the *reproducibility* of your work. I highly recommend it!





# Predicted values

You can generate predicted values from the data used to model using `predict()`. 
```{r, eval = FALSE}
# try it out
predict(therm_mod)
```

We can also use `predict()` to generate predictions for new datasets. This is good for visualizing the effect of one variable, holding others constant. You can do this by creating a "counterfactual dataset" and generating model predicts for the observations in the counterfactual data. 

```{r}
# we only need one variable
# be sure it has the same name as the regression variable

id_frame <- data_frame(libcon_self = 1:7)

# use the estimated model to predict for the new data

predict(therm_mod, newdata = id_frame)
```

We can also add intervals of various kinds (confidence intervals for means, prediction intervals, and so on). Let's generate predictions for the linear model and the "all intercepts" model, and then plot them side by side for comparison.

```{r}
# predictions from linear model, using the new id_frame data

lin_preds <- 
  predict(therm_mod, newdata = id_frame, interval = "confidence") %>%
  as_data_frame() %>%
  print() 

# predictions from intercepts model
# predict() is smart enough to factorize your predictor variable
#   if you factorized it in the model

int_preds <- 
  predict(int_mod, newdata = id_frame, interval = "confidence") %>%
  as_data_frame() %>%
  print()

# combine!
compare_mods <- 
  bind_rows(mutate(lin_preds, libcon_self = id_frame$libcon_self, 
                              mod = "linear"), 
            mutate(int_preds, libcon_self = id_frame$libcon_self, 
                              mod = "intercepts")) %>%
  print()
```


When you have these predictions as a data frame, you could plot!

```{r}
# plot points with dodging
# suppress minor grid

ggplot(data = compare_mods, aes(x = libcon_self, y = fit)) +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(ymin = lwr, ymax = upr, shape = mod),
                  position = position_dodge(width = 0.25),
                  fill = "white") +
  scale_x_continuous(breaks = 1:7) +
  scale_shape_manual(values = c(16, 21)) +
  coord_cartesian(ylim = c(-50, 50)) +
  labs(x = "Ideological Self-Placement",
       y = "Relative Candidate Thermometer\n(Republican minus Democrat)",
       shape = "Model") +
  theme(panel.grid.minor = element_blank())
```

I'll leave it to you to interpret these models and assess the pros and cons of each.




## Packages for model predictions

For more complex post-estimation graphics (e.g. first differences, marginal effects), there are some packages that make your job a little easier. I won't cover them all (because not everyone engages in these types of predictions), but I'll lay some out:

 - The `broom` package (where `tidy()` comes from) provides the `augment` function, for augmenting a data frame with predictions from an accompanying model. This works a bit like `predict()`, but it returns tidy data frames. `augment` gives you a "standard error of the fit" variable rather than separate variables for upper and lower boundaries, but you can use the standard error to calculate however wide a confidence interval you desire. 
 - The `margins` package by Thomas Leeper is meant to imitate the workflow of Stata's `margins` and `marginsplot` workflow. 
 - `sjPlot`
 - `Zelig`, a Gary King project, tries to standardize model evaluation and visualization across model types
 - `effects`, which is a little older but still commonly advocated

I prefer using `broom` because it is meant to work in a "tidy" workflow, but you have to do the work to generate your own critical value for generating a confidence interval. I don't find this to be an onerous task, since I believe you should be actively thinking about your critical values anyway. I don't use this kind of stuff much, however, because I tend to do uncertainty by simulation or with a Bayesian model. Needless to say, however, there are plenty of documents and examples on the web for these packages.






# Generalized linear models

Generalized linear models (GLMs, such as logit, poisson regression, negative binomial regression, and so on) are similar to linear models, but they are designed for nonlinear relationships. GLMs and LMs are similar, but the following key differences distinguish the two.

##  Distribution of the dependent variable

In a linear model, residuals are assumed to be normally distributed. Another way to write this is that $y_{i}$ itself is normally distributed around the expected value of $y$, conditional on $x$. Call this conditional mean $\mu_{i}$. 

$\begin{align} y_{i} &\sim \mathrm{Normal}\left( \mu_{i}, \sigma \right) \\[6pt] \mu_{i} &= \alpha + \beta x_{i} \end{align}$

This is how you write a linear model as a normally distributed $y$. For generalized linear models, it's the same basic idea, except the outcome distribution is non-normal (we'll use $\tau$ to indicate a dispersion parameter). 

$\begin{align} y_{i} &\sim \mathrm{Some \, Distribution}\left( \mu_{i}, \tau \right) \end{align}$

## Linearity

The other component that sets GLMS apart is the linearity of the relationship between $x$ and $y.$

- In a linear model: $x$ is assumed to have a linear effect on $\mu_{i}$. Coefficients describe how $x$ impacts $\mu_{i}$.
- In a GLM: $x$ has a linear impact on a *transformation* of $\mu_{i}$, which implies that $x$ is non-linearly related to $y$. Coefficients describe how $x$ impacts the transformed $\mu_{i}$.

So the full GLM is like the Normal model, but we use a different distribution, and $\mu_{i}$ isn't linearly related to $x$.

$\begin{align} y_{i} &\sim \mathrm{Some \, Distribution}\left( \mu_{i}, \tau \right) \\[6pt]  \mathcal{f}\left( \mu_{i} \right) &= \alpha + \beta x_{i} \\[6pt] \mu_{i} &= f^{-1}\left( \alpha + \beta x_{i} \right) \end{align}$


The transformation of $\mu_{i}$ is called a *link function*. We call these "generalized linear models" because there *is* a linear component, but it's a linear relationship between $x$ and $\mathrm{link}(\mu_{i})$. And it's *generalized* because a linear model fits into this framework as well, but the link function is simply $1 \times \mu_{i}$.



## An example using logit

We'll use logistic regression to predict a vote for the Republican presidential candidate using ideological self-placement and gender as covariates. 

Fitting this into the GLM framework...

- The outcome distribution is Bernoulli. The dependent variable $y$ is a 1 or a 0, "success" or "failure". 
- We don't model $y$ directly. Instead, we want to model $\pi_{i}$, which is the *expected value* of $y_{i}$. 
- The link function connecting $\pi_{i}$ to the regression equation is called the *logit* function, a.k.a. the "log odds" of $\y_{i}$. 

Here's the math. We'll use $y_{i}$ to indicate the observed vote for voter $i$, $\pi_{i}$ is the probability that $i$ votes for the Republican.

$\begin{align} y_{i} &\sim \mathrm{Bernoulli}(\pi_{i}) \\[6pt] \ln \left( \frac{\pi_{i}}{1 - \pi_{i}} \right) &=  \alpha + \beta_{1} x_{1i} + \beta_{2} x_{2i} + \ldots \end{align}$

As we can see, neither our data $y_{i}$ nor the expected value $\pi_{i}$ are linearly related to our predictors. Instead, the transformation of $\pi_{i}$ (the log odds) is linearly related to the predictors.

## Let's do this in R.

Here are the data from 1996 only.

```{r}
# vote, ideology, and gender data from 1996

logit_data <- anes %>%
  filter(cycle == 1996) %>%
  select(vote, libcon_self, gender) %>%
  print()
```

Let's transform this data to make it play nicely with modeling math.

```{r}
# convert rvote to a dummy (treat a logical as a number, 0 or 1)
# same with gender
# center the ideology scale on 4, so the constant (x = 0) represents moderates

logit_data <- logit_data %>%
  mutate(rvote = as.numeric(vote == "Republican Candidate"),
         woman = as.numeric(gender == "Women"),
         ideo = libcon_self - 4) %>%
  print()
```

You'll see `NA`s in the data. Cases with missing values are automatically dropped during estimation. You may cover missing data imputation in your maximum likelihood course.

The estimation formula in R looks like `lm()`, but we specify a family of probability distributions. We use "binomial," which is how you do logit. (Binomial is a Bernoulli for multiple observations). 

```{r}
# estimate the model with glm() and binomial distribution

vote_logit <- glm(rvote ~ ideo + woman + ideo*woman,
                  family = binomial(link = "logit"),
                  data = logit_data)

summary(vote_logit)
```

We can create coefficient plots and tables as before, so I won't demonstrate those. I will say that when you want to compare GLMs to one another, it is usually smarter to compare predictions or model fit statistics than it is to compare coefficients themselves. Because coefficients are on unintuitive scales (the "link scale") and sometimes involve ancillary parameters that help adjust the fit (such as cutoff parameters in ordinal logit), small changes to the model may change coefficients in ways that *look* large, but the effects on the actual predicted value may be negligible.

Visualizing the predictions from a GLM is similar to linear modeling, but we add one step.

- Create counterfactual data for simulated predictions
- Generate linear predictions using model coefficients
- *Transform linear predictions with inverse link function*
- Plot as desired

Here we only use coefficients to generate the linear predictions from the model. The predictions and bounds are on the link scale (log odds ratios). 

```{r}
# new data frame of values -3 through 3, 
#   which is the rescaled ideology scale

# for GLMs, the critical value is always 1.96 
#   (assumes normal coefficients on the link scale)

# here are predictions on the link scale (for logit: the log-odds scale)
# using the augment() function from the broom package

logit_preds <- data_frame(ideo = rep(-3:3, 2),
                          woman = c(rep(1, 7), rep(0, 7)),
                          `ideo:woman` = ideo * woman) %>%
  augment(vote_logit, newdata = .) %>%
  mutate(lower = .fitted - (1.96 * .se.fit),
         upper = .fitted + (1.96 * .se.fit)) %>% 
  print()
```

If we don't transform the linear predictions, then we get predictions on the link scale (the log odds scale in a logit model). That's how we can have negative predicted values, for example. We could plot them, and they'd look like straight lines (just like OLS), but log odd ratios are hard to interpret. Instead, we will transform the log odds to *predicted probabilities* using the `plogis()` function, which is the inverse of the logit link function (a.k.a. the "logistic function," which happens to be the cumulative distribution function of the logistic distribution).

```{r}
# transform log odds to probabilities

logit_preds <- logit_preds %>%
  select(-.se.fit) %>%
  # mutate only the selected variables
  mutate_at(vars(.fitted, upper, lower), plogis) %>%
  print()

# modify data and plot

 
ggplot(logit_preds, aes(x = ideo, y = .fitted)) +
  geom_pointrange(aes(ymin = lower, ymax = upper, 
                    color = as.factor(woman)),
                position = position_dodge(width = 0.5),
                show.legend = FALSE) +
  annotate("text", x = 0.5, y = .59, label = "Men") +
  annotate("text", x = 1.7, y = .59, label = "Women") +
  scale_x_continuous(breaks = -3:3,
                  labels = c("Very\nLiberal", 
                             "Liberal", 
                             "Slightly\nLiberal", 
                             "Moderate", 
                             "Slightly\nConservative", 
                             "Conservative", 
                             "Very\nConservative")) +
  labs(color = NULL,
     x = "Ideological Self-Placement",
     y = "Probability of Republican Vote") +
  theme(panel.grid.minor = element_blank())
```

Note how the predictions are non-linear. The predictions approach 0% and 100% but never exceed them. GLMs are useful because they accurately capture these sorts of ceiling and floor effects.

There are *loads* of GLMs out there, because there are loads of ways your data aren't perfectly normal and linearly related. But they all fit the same basic framework: $y$ follows some distribution, and you need some link function to describe how $y$ is related to $x$.





# Model diagnostics

The model objects created by `lm()` and `glm()` do include some model diagnosis tools, such as F-statistics, $R^{2}$ values, deviance, AIC, so on. We'll walk through some here.

Some diagnostics for linear models can be easily visualized using `plot()`, including quantile plots, and analyses of residuals.

```{r}
plot(therm_mod)
```

There are certain functions and packages that can be used to generate model fit statistics. I think the easiest tool for comparing models is `broom::glance()`. Just like `broom::tidy()`, you can stack the data frames created by `glance()` to compare models easily or plot the statistics.

```{r}
bind_rows(mutate(glance(therm_mod), mod = "LibCon"), 
          mutate(glance(dummy_mod), mod = "Dummies"),
          mutate(glance(int_mod), mod = "Intercepts"))
```

This works for GLMs as well, but maximum likelihood models have some different fit statistics than least-squares models. If you want to compare linear and nonlinear models, you could estimate the linear model as a GLM model of Gaussian family with an "identity" link.

```{r}
glance(vote_logit)
```


Some additional tips and tools.

- Some of these diagnostics will show an improvement in model fit even if the improvement comes from fitting noise (such as $R^{2}$). These diagnostics are statistics that take a distribution, so you want to compare models using a statistical comparison---i.e. is the fit improvement *enough* given that you've added an extra variable to the model. Examples include F-tests and likelihood-ratio tests. If you go down this route, you might check out tools such as `epicalc::lrtest()` or the `lmtest` package. Other packages for model assistance (such as `arm` or `rms`) may have similar tools as well. Other diagnostic measures will penalize you for adding variables on the front-end, such as BIC, so they don't require formal statistical tests. 
- My advice is that if you want to be doing this kind of intense model comparison, make sure you know what these statistics are checking and that the use is appropriate for your task at hand. There really are no hard and fast rules here, so you want to do what makes sense for your use case. 
- Out-of-sample prediction is a good test for model over-fitting. This can be evaluated using cross-validation. The `loo` package provides tools for easier CV performance. (Also, the AIC is intended to estimate out-of-sample model accuracy).
- Simulating artificial data can be a useful face-validity check. If you are estimating a generative model of your data (and you are...), the model should generate data that look like your data. 




# Intermediate R tricks

Now we will quickly introduce some more nitty-gritty R tricks. These may not be essential for the problem set, but over the long run, you will be a much more efficient R user if you take these concepts seriously.


## Type coercion

As we covered early in the course, there are a few different data types in R: logical, numeric, factor, and character. Data can be *coerced* from one type to another with `as.type()` functions, where `type` refers to the resulting data type.

Let's start as broad as possible with characters. As the broadest of these data types, anything can be coerced to a character.

```{r}
# logical to character
as.character(TRUE)
# numeric to character
as.character(c(1, 2, 3))
# factor to character
F <- factor(c("hello", "world"))
as.character(F)
```

Converting to factor is similar. Each unique value is given its own factor level, and the order of levels is assigned alphabetically unless specified with `factor(..., levels = c(...))`. 

```{r}
as.factor(c(TRUE, FALSE))
as.factor(c(1, 2, 3))
# note the level order
factor(c("1", "2", "3"), levels = c("3", "2", "1"))
```

Converting to numeric is slightly more confusing. Logical variables are easy and can be converted to `1`a and `0`s. 

```{r}
as.numeric(c(TRUE, FALSE))
```

Factors also work, but the coercion gives numeric meaning to the underlying factor labels. 

```{r}
# levels assigned in reverse order
(F <- factor(c("hello", "world"), levels = c("world", "hello")))
# note the mapping to numeric...
# Making a data frame to visualize
data_frame(factor = F, 
           numeric = as.numeric(F))
```

Character vectors cannot be directly mapped to numeric. They need to be converted to factor first.

```{r}
char <- c("a", "b", "z")
# direct coercion leads to NAs
as.numeric(char)
# coercion through factor works
as.numeric(as.factor(char))
```


Here's where you typically see these forms of coercion.

- using logicals to convert categorical variables into dummy variables
- using `as.factor()` to convert a numeric index into a set of dummy variables in a regression function
- converting numeric caseID variables to character in order to fix any problems with leading zeroes. This is common with geocodes like FIPS codes.
- Converting character vectors to factors for plotting (placing categories in order for legends or panel titles). 


## User-defined functions

In R, we can write our own functions to perform repetitive tasks. Let's demonstrate the a task for finding a mean.

```{r}
my_mean <- function(x) {
  
  the_sum <- sum(x) 
  n <- length(x)
  the_mean <- the_sum / n 
  
  return(the_mean)

}

z <- 1:5
my_mean(z)
```

User-defined functions have three components.

- The function name, which is what we assign the function to.
- Arguments, passed to the function, manipulated within the function
- The definition, which details how arguments are manipulated and what the function returns

It is important to note that the variables inside the function definition are called *local variables*. This means they only exist in the world of that function. They are not accessible elsewhere in R. In the above example, `x`, `the_sum`, `n`, and `the_mean` are manipulated by the function but are not available to you to play with. Furthermore, if there are other objects currently in R memory that share those same names, they have no bearing on how the function works. Local variables help define a function and perform its intended purpose, but they do not affect and are not affected by the other objects in your current R workspace.



## Lists

There is one data structure that we have not yet discussed: lists. Lists are like vectors, but unlike vectors, their elements can be of any data type. Let's demonstrate. 

```{r}
# create a list of named elements
el <- list(num = 1, 
           fact = factor(c("a", "b", "c")),
           char = c("x", "y", "z"))
# check it out
el

# numeric indexing gives you the named element (including the name)
el[1]

# to get all the way down to the data...
el$num
el[[2]]
el[["char"]]


```

This can be handy for stacks of data frames. For example, the `anes` dataset, but element is a data frame corresponding to each survey year.

```{r, eval = FALSE}
# This will print a big monstrosity, 
# but you should see what it looks like
anes_list <- split(anes, anes$cycle)
```


## Functional programming with `apply()` functions.

You'll see stuff about `apply()` functions online. They are scary at first, but they make sense if you give them a chance. 

Let's see what we mean. Let's create a two-D object.

```{r}
df <- data_frame(a = 1:5, b = a, c = a) %>%
  print()
```

An `apply()` function takes an object, and applies a function across its dimension(s). This is easier to explain using an example: here, we will apply the `mean` function to the rows and columns of this `df` object.

```{r}
# 1 = row
# get the mean of every row
apply(df, 1, mean)
```

It returns an object the same length as the number of rows in the object, containing the result of the `mean()` function for each row. 

Here it is for each column.

```{r}
# 2 = columns
# get the mean of every column
apply(df, 2, mean)
```

You could pass a user-defined function to `apply()`, or you could define a function within `apply()` using "anonymous functions." Example:

```{r}
df 
# apply x * x to every column
apply(df, 2, function(x) {x * x})
```

This anonymous function applies the function `x^2` to each column in `df`. 

There are a few types of apply functions, but you're only likely to use a few of them. 

- `apply`: apply a function over the margins of an object
- `sapply`: simplify the `apply()` results to a one-D vector, if possible
- `lapply`: apply for each element in a list

Here is an `lapply` example, using the `anes_list` object we created above. We'll use an anonymous function to find the mean `party_distance` in each election cycle.

```{r, eval = FALSE}
# 'x' refers to each element of the list
# each element being a data frame
# so x$var finds 'var' in the x data frame

lapply(anes_list, function(x) mean(x$party_distance, na.rm = TRUE))
```

From there, you can do cool things like "melt" the list into a data frame using `reshape2::melt()`.

```{r, eval = FALSE}
lapply(anes_list, function(x) mean(x$party_distance, na.rm = TRUE)) %>%
  reshape2::melt()
```



## Nesting: superpowered lists 

When you get *really* comfortable with function programming, you can do crazy stuff like nest a data frame. What is that?

```{r}
anes %>%
  group_by(cycle) %>%
  nest() 
```

A nested data frame is a data frame where columns can themselves be a list of data frames (a.k.a. a "list column"). In this data frame, the `data` column isn't really a variable; it contains a list of data frames, each corresponding to the grouping variable (`cycle`). 

Unnest a list column from a data frame like so: 

```{r, eval = FALSE}
anes %>%
  group_by(cycle) %>%
  nest() %>%
  # unnest the `data` column
  unnest(data)
```

Why is this useful? Well...

## Mapping a function over a list column

This is another functional programming trick, like apply, but applied to a list column in a nested data frame.

Let's say we had the above nested frame (a data frame for each survey wave), but we wanted to estimate a regression for separate data frames. 

Here, we estimate the effect of gender on Republican voting using `purrr::map()`, which is like `apply()` but it works across a list column in nested data frame.

```{r}
# nesting the data
#   get indicator for R vote
#   removing NAs
#   group by cycle and nest
# Apply function over the list column
#   use every data frame in the list column
#   run glm() using an intercepts model for comparisons (no constant)
#   tidy the model output
# Unnest the data
#   unnest the results of map()
#   create a gender label

gender_gaps <- anes %>%
  mutate(rvote = as.numeric(vote == "Republican Candidate")) %>% 
  filter(!is.na(rvote) & !is.na(gender)) %>%
  group_by(cycle) %>%
  nest() %>% 
  mutate(model = map(data, 
                     ~ glm(rvote ~ -1 + as.factor(gender), 
                           data = ., family = "binomial") %>%
                       tidy(conf.int = TRUE))) %>%
  unnest(model) %>%
  mutate(term = case_when(str_detect(term, "Men") ~ "Man",
                          TRUE ~ "Woman")) %>% 
  print()

# # plot coefficients over time
# # map pt shape, solid and empty points, generic white fill
ggplot(gender_gaps, aes(x = cycle, y = estimate, color = term)) +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high,
                      shape = term),
                  fill = "white",
                  position = position_dodge(width = 0.5)) +
  scale_shape_manual(values = c(16, 21)) +
  scale_x_continuous(breaks = seq(1948, 2012, 8)) +
  labs(y = "Effect on Republican Voting (Log Odds Scale)",
       x = "Election Cycle",
       color = NULL, shape = NULL)
```

Mapping is a tool that takes some getting used to at first. In particular, you have to get a feel for the formula syntax where a function follows a `~` symbol, and you use `.` to represent element names in the `data` column. Once you get this down, however, mapping is an extremely powerful tool for scaling up analysis because not only can you do a lot of repetitive work with very little code, the code is executed using parallel processing when possible. This makes it much faster because it distributes tasks across processing cores on your computer (nice!). 




# References for advanced topics

As you develop your substantive scholarly interests, it is likely that you will develop a methodological expertise to fit your topic of study. Luckily, many others have come before you and have developed R tools for doing these analyses. Better yet, these computational tools are being increasingly folded into a `tidyverse`-style tools. We'll quickly point out a few of these resources. You will *NOT* be required to use these tools for any take-home exercises.

Some higher-level advice for navigating these packages:

- My philosophy is that I like to rely on external packages for computation and estimation, but not for graphics. If there is a tool that estimates a model for me, or performs a particular statistical test, then that's great. But I tend not to like the graphics that these tools produce. *As a result*, I look for tools that make it easy for me to extract the data that I want to plot. 
- Sometimes it is tedious to extract the data from these objects. In these situations, I tend to write my own functions to process the output from these packages into a tidy format for plotting or tabulating.
- If you want a quick and easy way to learn about packages, make a Twitter for your "academic self" and follow some researchers and R developers.


## Survey analysis

The statistics that we learn apply to data collected from simple random samples. In the real world, however, survey data often require some kind of clustered sample design and contain accompanying sample weights. Analyzing surveys requires (or, should require) accounting for weights and design as separate elements of the analysis.

If you have a non-clustered sample design but some degree of oversampling, you might handle weights analytically---calculating weighted means and weighted sample sizes. If you have a more advanced sample design, you should incorporate elements of the design into the estimation. To that end, I'd recommend Thomas Lumley's [`survey`](https://cran.r-project.org/web/packages/survey/) package. You use your dataset to create a new object that contains metadata about the cluster structure of the sample. Functions in the `survey` package then use the metadata about the sample design to estimate things properly. This is similar to the way you can declare survey design information using `svy`-based commands in Stata.


## Time series

For time series, you will want some special tools to deal with the accompanying statistical pitfalls: functional forms for autocorrelated errors, standard errors for autocorrelation, and the estimation of ancillary parameters for models designed for certain temporal interventions. 

First, for data manipulation, you will want some kind of data structure that contains metadata about which variable defines the time period. This structure will allow you to properly calculate differenced variables, lags, and leads. To create tidy, time-aware tibble datasets, you could use [`tibbletime`](https://github.com/business-science/tibbletime) or the more recent (and supposedly more capable) [`tsibble`](http://pkg.earo.me/tsibble/index.html). You could also check out the `lubridate`, `zoo`, and `hms` (Tidyverse!) packages for manipulating data-time variables, since the baked in R tools for dealing with `POSIXct` and `POSIXlt` data are very difficult to figure out. If you read that sentence and were like "wtf are `POSIXct` and `POSIXlt`?", that's exactly what I mean.

For time series *modeling*, you will want tools that perform a variety of functions.

- ARIMA modeling
- Unit root and (fractional) integration testing
- modeling for interventions, autoregressive distributed lag (ADL), error-correction (ECOM) vector autoregression (VAR), granger causality tests, impulse-response functions, and so on

I don't have expert-level advice here, but when I took our time series ITV course, I found the following packages useful for several of these needs: `TSA`, `fUnitRoots`, `egcm`, `fracdiff`, `forecast`. 



## Panel data

Panel data tends to be the realm of "fixed-effects" modeling, meaning that when you measure features over time, time-invariant predictors are absorbed into fixed unit-level averages, and time-varying features have coefficients that are constant across time. I don't typically do this kind of analysis, but those who do often use the `plm` package for these types of models. 

Alternatives to `plm` include hierarchical modeling approaches, which we'll cover in a separate subsection.


##  Hierarchical/multilevel models

For complex hierarchical data structures (individuals within time periods, individuals within geographic groups, observations within countries within regions within time periods...), hierarchical models may more be a more direct modeling approach to attributing variation in the data to covariates at different levels of analysis without as much scrutiny about clustered variance estimators and so on. This is because hierarchical modeling allows you to directly model parameter estimates as functions of covariates at other levels of the data. For example, the probability that an individual votes Republican may be a function of their demographic characteristics but also the context of state the state in which they live.

$\begin{align} y_{i} &\sim \mathrm{Bernoulli}\left( \pi_{i} \right) \\[6pt] \mathrm{logit} \left( \pi_{i} \right) &= \alpha + \gamma^{\mathtt{demographics}}_{j[i]} + \delta^{\mathtt{state}}_{s[i]} \end{align}$

The state effect applies to every individual in that state and could itself be a regression on state-level features such as the presidential vote in the state, state-level economics, and so on.

$\begin{align} \delta^{\mathtt{state}}_{s} &\sim \mathrm{Normal}\left( \beta_{1} \mathrm{pvote}_{s} + \beta_{2} \mathrm{GDP}_{s} + \ldots , \, \sigma^{\mathtt{state}} \right)\end{align}$

This kind of modeling is useful because it allows estimates for small groups to "borrow strength" from larger groups. If we don't have a lot of data for Alabama, for example, we can say that Alabama is probably like other states that have similar state-level characteristics, and it shrinks Alabama's estimate toward the state regression trend. In other words, for small-$n$ groups, we assume that the group-level effect looks like the other group-level effects *unless* the data give us a strong signal to the contrary. This is a key example of the bias-variance trade-off you heard about in stats courses.

Although hierarchical models are "essentially Bayesian" because of the partial pooling setup, there are packages for fitting approximate maximum-likelihood versions. The most common would be `lme4`, which provides [syntax](https://stats.stackexchange.com/questions/18428/formula-symbols-for-mixed-model-using-lme4) similar to `nlme` for varying ("random") effects, but it is more updated than `nlme`. What I'm saying is, don't use `nlme`. The `arm` package provides additional tools for interacting with `lme4` hierarchical models, including the `bayesglm` function that just says "screw-it" and fits the fully Bayesian version of the model. On that subject...


## Bayesian analysis

Bayesian analysis varies from "frequentist" statistics in a few fundamental ways. The main source of difference is philosophical, where uncertainty estimates are understood as your uncertainty about the actual value of the parameter, and not uncertainty about the *data*. Stated differently, frequentism measures the *probability of the data* given an assumed model of null parameter values and infinitely repeated sampling. Bayesian statistics rejects the idea of the null model entirely and instead measures the *probability parameter values* after having observed the data, which requires prior information over the parameter values. When it comes to the actual parameter estimates, you can think about maximum likelihood models as being *special cases of Bayesian models* where the researcher inserts no prior information about the parameter values. 

There are a few ways to fit Bayesian models. For reduced-form regression models (like `lm` and `glm` functional forms), you can use packages such as `arm`, `brms`, and `rethinking` to write Bayesian models using a [syntax](https://stats.stackexchange.com/questions/18428/formula-symbols-for-mixed-model-using-lme4) similar to `glm` and `lme4` models.

For complicated structural models that are not easily expressed in a single regression equation (e.g. when you have a complex multi-level structure), you can may want to set up a fully Bayesian model using external Bayesian modeling software that can be accessed by R. For simpler models, one could use `JAGS`, which samples a posterior distribution using a Gibbs sampling algorithm. You would use the `rjags` package to talk to JAGS using R. For more complex hierarhical models, randomly-walking algorithms for Gibbs sampling (like `JAGS`) do a poor job, so I recommend using `Stan` (and talking to it with R using the `rstan` package). Stan fits the model using a version of Hamiltonian Monte Carlo, both of which drastically increase the speed and quality of posterior sampling. The `Stan` syntax is more complicated than `JAGS`, but the payoff of using `Stan` is worth it. 

For diagnosing and visualizing Bayesian model results, `rstan` has some tools baked in. The `ggmcmc` package turns posterior samples into a tidy data frame (good for ggplot!), and `bayesplot` provides other tools for easy Bayes graphics.


## R as front-end

As the Bayes packages indicate, R can serve as a front-end interface to other programs and syntaxes. Some further examples include the following packages...

- `rsql` and `RSQLite` for SQL and SQLite
- `Rcpp` for C++
- `rPython` for Python

...and so on


## More materials from past years

Sarah Bouchat (former instructor for this course) has online materials for some additional topics, including text analysis, Regular Expressions (RegEx), base graphics, loops, and so on. (I purposefully don't teach loops because `apply()` functions are better!) 

View Sarah's site [here](https://bouchat.github.io/553). 

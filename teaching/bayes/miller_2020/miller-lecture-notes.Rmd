
Big things to hit:

- Philosophy of Bayes
    - point estimators vs. posterior distribution
    - confidence interval vs. Bayesian interval
    - fixed and random parameters
    - joint probability routine (nothing magic about Bayesian inference. It just asks a different question)
- Simple model
    - Bayesian notation
    - Thinking about priors
        - what should a prior "do"? 
        - Stack probability in certain regions
        - Have a desirable curvature
        - have certain "entropic" properties
- Advantages of Bayesian analysis
    - Constrain parameters (practically)
    - Functions of parameters without analytical pain (but we do have Monte Carlo error)
    - _Partial pooling_
    - Sensitivity testing

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog Posts on Michael DeCrescenzo</title>
    <link>/post/</link>
    <description>Recent content in Blog Posts on Michael DeCrescenzo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael DeCrescenzo {year}</copyright>
    
	    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Displaying Git information in your open-source research paper (with Rmarkdown)
</title>
      <link>/post/git-in-papers/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/git-in-papers/</guid>
      <description>
&lt;style type=&#34;text/css&#34;&gt;
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
&lt;/style&gt;


&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;One benefit of open-source research is that it is possible to trace the history of a research product through its (potentially many) iterations using a versioning system such as Git. This is great for readers who encounter the project’s remote repository, but it’s more likely the case that readers will encounter only a PDF of your paper in an email or through a preprint archive. While services like ArXiv will watermark your paper, it (or so it seems) only includes information about the paper’s history in ArXiv specifically, rather its history in your Git repository. This post describes how you can use Rmarkdown to include Git information into a working draft of your research paper.&lt;/p&gt;
&lt;p&gt;What exactly do I mean? Your paper typically includes the date of compilation, but you could also include the current commit hash, the branch of the current commit, and so on. Why would you want to do this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A compilation system like &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\LaTeX}\)&lt;/span&gt; can print the date of compilation, but it is often the case that documents are re-compiled without any real changes. This means the compilation date can be a deceiving signal about when the paper was most recently modified. You may want to “timestamp” a version of your paper in a way that is robust to re-compilation at an arbitrary future time.&lt;/li&gt;
&lt;li&gt;As you develop your paper locally, you may commit several small changes between major versions of your paper. To prevent your “in-development” copy from being confused for a major version of the paper, you may want to note which commit generated the current PDF and perhaps link to a more stable “for public eyes” version of the paper elsewhere.&lt;/li&gt;
&lt;li&gt;A more general case of the previous point: suppose you develop your project across multiple branches (e.g. as with &lt;a href=&#34;https://datasift.github.io/gitflow/IntroducingGitFlow.html&#34;&gt;“Git flow”&lt;/a&gt;). You may reserve your “master” branch for major versions of the project while iteratively developing the project (and compiling the document) on a non-master branch. In this case, you might want to know if a PDF was compiled from source code on the master branch (i.e. “Am I looking at a major version of the paper”) or on an in-development branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example from one of my in-progress papers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/for-posts/git-date.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;This setup prioritizes the commit hash over the compilation date as a method for “dating” your paper. The branch name is included in cases where the PDF is generated on a development branch instead of on the master/public branch. The footnote corresponding to the commit information contains the commit message (not shown). And lastly, the link to the public version takes you to the master branch PDF on Github—the most recent major version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-do-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to do it&lt;/h1&gt;
&lt;p&gt;Setting this up consists of essentially two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Learn to print Git commands to the console using R.&lt;/li&gt;
&lt;li&gt;Place that R code in your &lt;code&gt;.Rmd&lt;/code&gt; document’s YAML header.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;console-commands-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Console commands with R&lt;/h2&gt;
&lt;p&gt;We can run console commands within R using the &lt;code&gt;system()&lt;/code&gt; function. Ordinarily the results of the commands merely print to the console instead of being treated as objects, but we want to make these objects be accessible in the R environment using the &lt;code&gt;intern = TRUE&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;Here are some examples that will display Git information for my website repo (where this code is currently being evaluated).&lt;/p&gt;
&lt;p&gt;For instance, how can we print the branch name?&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;kw&#34;&gt;system&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;git symbolic-ref --short HEAD&amp;quot;&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;intern =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;dev&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print only the hashes in your Git log, you can supply &lt;code&gt;%t&lt;/code&gt; to the the &lt;code&gt;--pretty&lt;/code&gt; argument of &lt;code&gt;git log&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;kw&#34;&gt;system&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;git log --pretty=%t&amp;quot;&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;intern =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;471d45e&amp;quot; &amp;quot;32e03b8&amp;quot; &amp;quot;d55b641&amp;quot; &amp;quot;175df3e&amp;quot; &amp;quot;03985bd&amp;quot; &amp;quot;549e2f0&amp;quot; &amp;quot;8effeb6&amp;quot;
##  [8] &amp;quot;e7c1fc3&amp;quot; &amp;quot;19f3bcd&amp;quot; &amp;quot;0647521&amp;quot; &amp;quot;5913357&amp;quot; &amp;quot;b146ac2&amp;quot; &amp;quot;494f860&amp;quot; &amp;quot;557bf2a&amp;quot;
## [15] &amp;quot;2b367c7&amp;quot; &amp;quot;734e099&amp;quot; &amp;quot;8ef25d4&amp;quot; &amp;quot;1d949ce&amp;quot; &amp;quot;ed14db3&amp;quot; &amp;quot;ba4694c&amp;quot; &amp;quot;57d5fc6&amp;quot;
## [22] &amp;quot;1656482&amp;quot; &amp;quot;28d68d7&amp;quot; &amp;quot;5b8e92a&amp;quot; &amp;quot;a807aab&amp;quot; &amp;quot;359f06a&amp;quot; &amp;quot;78c3ee3&amp;quot; &amp;quot;defc14f&amp;quot;
## [29] &amp;quot;ec7e081&amp;quot; &amp;quot;e4c9176&amp;quot; &amp;quot;ab502db&amp;quot; &amp;quot;7fe3ee6&amp;quot; &amp;quot;2f97534&amp;quot; &amp;quot;3259f27&amp;quot; &amp;quot;bec13bd&amp;quot;
## [36] &amp;quot;f3142cc&amp;quot; &amp;quot;2959bf6&amp;quot; &amp;quot;b4754c2&amp;quot; &amp;quot;91fe96a&amp;quot; &amp;quot;91bba9b&amp;quot; &amp;quot;071d153&amp;quot; &amp;quot;8e4cce3&amp;quot;
## [43] &amp;quot;ba09b95&amp;quot; &amp;quot;741632b&amp;quot; &amp;quot;3569cdc&amp;quot; &amp;quot;d99c163&amp;quot; &amp;quot;5c135e3&amp;quot; &amp;quot;2671a4b&amp;quot; &amp;quot;2b7d810&amp;quot;
## [50] &amp;quot;ea7d44d&amp;quot; &amp;quot;6c7656c&amp;quot; &amp;quot;e40d5d8&amp;quot; &amp;quot;bb9199d&amp;quot; &amp;quot;ca4e593&amp;quot; &amp;quot;c42c33f&amp;quot; &amp;quot;d17291e&amp;quot;
## [57] &amp;quot;38d1910&amp;quot; &amp;quot;6bc2299&amp;quot; &amp;quot;3131d9d&amp;quot; &amp;quot;5906234&amp;quot; &amp;quot;d355f02&amp;quot; &amp;quot;7a6e215&amp;quot; &amp;quot;c5befba&amp;quot;
## [64] &amp;quot;b0dba1c&amp;quot; &amp;quot;c1d6342&amp;quot; &amp;quot;87f3ceb&amp;quot; &amp;quot;83ca75b&amp;quot; &amp;quot;69e41cf&amp;quot; &amp;quot;f9278c7&amp;quot; &amp;quot;a3ee86e&amp;quot;
## [71] &amp;quot;816ebb5&amp;quot; &amp;quot;030278d&amp;quot; &amp;quot;2d9384b&amp;quot; &amp;quot;fec8391&amp;quot; &amp;quot;83dbb8c&amp;quot; &amp;quot;1210553&amp;quot; &amp;quot;ce35ec0&amp;quot;
## [78] &amp;quot;ab3c776&amp;quot; &amp;quot;c62ad9f&amp;quot; &amp;quot;3148687&amp;quot; &amp;quot;c3621d8&amp;quot; &amp;quot;943687e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use indexing to isolate only the most recent hash from this vector of results.&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;kw&#34;&gt;system&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;git log --pretty=%t&amp;quot;&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;intern =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;)[&lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;471d45e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print the commit message, use &lt;code&gt;--pretty=%s&lt;/code&gt; instead.&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;kw&#34;&gt;system&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;git log --pretty=%s&amp;quot;&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;intern =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;)[&lt;span class=&#34;dv&#34;&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;categorize post&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;r-results-in-the-yaml&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R results in the YAML&lt;/h2&gt;
&lt;p&gt;Now that we know which commands to run to get the Git info, how do we get this information into our YAML? We will do this using inline R code chunks. This image shows what I’ve done for the above paper example, and I describe a few of the tricks I use below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/for-posts/git-yaml.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We use the &lt;code&gt;date&lt;/code&gt; variable, but we supply multiple lines of content. To do this, place a pipe &lt;code&gt;|&lt;/code&gt; after declaring the &lt;code&gt;date&lt;/code&gt; variable, and begin each line with a new pipe &lt;code&gt;|&lt;/code&gt;. This will line-break the content in your compiled PDF and let you supply &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\LaTeX}\)&lt;/span&gt; code directly to the variable.&lt;/li&gt;
&lt;li&gt;To use teletype/fixed-width font, type the &lt;code&gt;\texttt{}&lt;/code&gt; command for &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\LaTeX}\)&lt;/span&gt; directly in Rmarkdown.&lt;/li&gt;
&lt;li&gt;We can evaluate and print the results of inline R code by including the letter &lt;code&gt;r&lt;/code&gt; at the beginning of an inline code chunk (delimited by backticks). This code is evaluated before the document is compiled, so the information being passed to &lt;code&gt;\texttt{}&lt;/code&gt; is the &lt;em&gt;results&lt;/em&gt; of the R code rather than the text of the R code itself.&lt;/li&gt;
&lt;li&gt;Do the same basic setup for the commit hash, commit message (in a footnote), and the compilation date. Note that the formatting of the compilation date gives you prettier results than the Rmarkdown default.&lt;/li&gt;
&lt;li&gt;Lastly, you can link the reader to the most recent public PDF by linking to your remote master branch. By linking directly to Github (or wherever else you host the remote repository), any time you push an update to remote, your PDF will automatically be up to date. This will be true of any offline PDF, any previous PDF, and any PDF generated on any branch. This is because the URL to your master branch PDF will not change even if the PDF file itself changes!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;caveat&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Caveat&lt;/h1&gt;
&lt;p&gt;When you push to Github, it creates new hashes that differ from your local machine. As a result, you can’t use the hash in the PDF to cross-reference the same hash on Github. This is a current shortcoming in my approach, and I expect to update this post soon once I figure out a system for incorporating the remote hash in addition to the local hash.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Causal Mediation, Bayesianly (with Stan)</title>
      <link>/post/bayes-mediation/</link>
      <pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes-mediation/</guid>
      <description>


&lt;div id=&#34;motivation-for-this-post&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation for this post&lt;/h1&gt;
&lt;p&gt;Over this summer, I have been organizing a reading group on causal inference for students in my department. As someone who sees data analysis problems primarily through Bayesian goggles, I have been doing extra work in my head to make sense of “Bayesian causal inference.” I’m hoping to write some articles about this for political scientists, but the dissertation (rightly) has more of my attention lately.&lt;/p&gt;
&lt;p&gt;We covered causal mediation this week (&lt;a href=&#34;https://imai.fas.harvard.edu/research/files/mediationP.pdf&#34;&gt;Imai et al. 2011 &lt;em&gt;APSR&lt;/em&gt;&lt;/a&gt;), which I thought would be a good opportunity to explain where my thoughts are going about this. So this post will briefly describe a Bayesian vantage point on causal inference and show how to use Bayesian tools to implement it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-draws.-i-mean-unobserved-potential-outcomes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Posterior Predictive Draws. I mean, “Unobserved Potential Outcomes”&lt;/h1&gt;
&lt;p&gt;It should be noted up front that a Bayesian take on causal inference is not at all new (I will borrow plenty of intuition from, for example, &lt;a href=&#34;https://projecteuclid.org/download/pdf_1/euclid.aos/1176344064&#34;&gt;Rubin 1978&lt;/a&gt;), but it is pretty unfamiliar to the political science/econ folks I roll with.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;
People often ask me, “How can you even have a Bayesian experiment; don’t you already have randomization?” as if the purpose of priors is to fix confounding somehow. In fairness to non-Bayesians, if this is how Bayesian analysis used priors, I would also be mistrusting of Bayes. Luckily, priors are less presumptuous than that. You get a Bayesian experiment (or any other credible research design) by specifying priors on the parameters and obtaining a posterior distribution. It is pretty unremarkable—no different than a Bayesian analysis of a non-causal design. Remember that the causal model (by which I mean, the definition of the potential outcomes) is distinct from the methods used to &lt;em&gt;estimate&lt;/em&gt; causal parameters. Bayesian analysis is positioned closer to the estimation end of things, whereas causal modeling is a series of assumptions about identifying variation in the data. In short, you fix confounding with the design, and priors are for improving the estimation.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;While the Bayesian approach may not change the research design or the causal assumptions, it does provide a different—and intuitive, I assert—interpretation of potential outcomes. Ordinarily we write potential outcomes as &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}(T_{i} = t)\)&lt;/span&gt;, the outcome value for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; if it received treatment value &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Only one potential outcome per unit is ever observed, so can’t observe the &lt;em&gt;unit-level&lt;/em&gt; causal effect &lt;span class=&#34;math inline&#34;&gt;\(\tau_{i}\)&lt;/span&gt;, but we can use a causal identification analysis to lay out the assumptions required to estimate an average effect &lt;span class=&#34;math inline&#34;&gt;\(\bar{\tau}\)&lt;/span&gt; for at least some subset of units. If we knew this average effect, we would be able to state, for each observed outcome &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;, what the &lt;em&gt;expected value&lt;/em&gt; of that unit’s unobserved potential outcome would be if we could set &lt;span class=&#34;math inline&#34;&gt;\(T_{i}\)&lt;/span&gt; to some value &lt;span class=&#34;math inline&#34;&gt;\(t&amp;#39;\)&lt;/span&gt; other than what was observed. In this way, the unobserved potential outcome is missing data that we can predict with an estimated the model that generates (potential) outcomes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Maybe you can see where I’m going with this.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bayesian analysis begins with joint model &lt;span class=&#34;math inline&#34;&gt;\(p\left(y, \theta \right)\)&lt;/span&gt; for outcome data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and model parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This is equivalently expressed as &lt;span class=&#34;math inline&#34;&gt;\(p(y \mid \theta)p(\theta)\)&lt;/span&gt;, which is to say that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; depends on the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; has its own distribution. We fit the model by conditioning on the observed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to obtain the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p\left(\theta \mid y \right)\)&lt;/span&gt;. It is this updated model that represents our state of information about the process that generates potential outcomes &lt;span class=&#34;math inline&#34;&gt;\(y_{i}(t)\)&lt;/span&gt;. If we wanted to make posterior inferences about what &lt;span class=&#34;math inline&#34;&gt;\(y_{i}(t)\)&lt;/span&gt; &lt;em&gt;would have been&lt;/em&gt; (in expectation) if we could arbitrarily change &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, we would simulate the unobserved potential outcomes &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt; from the model.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p(\tilde{y} \mid y) &amp;amp;= \int p(\tilde{y} \mid \theta) p(\theta \mid y)d\theta
\end{align}\]&lt;/span&gt;
The unobserved potential outcome is expressed as a probability distribution because we don’t know exactly what the unobserved data would be. Its distribution depends on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which itself is conditioned on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and we average over our uncertainty about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; by integrating. This gives us a distribution for the unobserved potential outcomes that is marginal of our imperfectly estimated parameters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Okay, so?&lt;/em&gt;&lt;/strong&gt; The Bayesian view of potential outcomes is appealing because our state of ignorance about the exact potential outcomes is an explicit feature of the model, rather than a point estimate with a post-hoc standard error. Which is to say, &lt;em&gt;we don’t know&lt;/em&gt; what the treatment effect is, and so we don’t know what the potential outcomes are, but we have a range of guesses that that we can directly evaluate using their probability distribution. This approach has a certain philosophical resonance before we get anywhere near the notion of prior information. And to whatever extent researchers already view point estimates and frequentist confidence intervals on treatment effects as “ranges of plausible values” with associated posterior probabilities, they are already doing Bayesian causal inference—just without the benefit of having formally set up the whole model. With the Bayesian approach we are actually allowed to say things like “the data suggest that this treatment effect is most likely positive” or what have you.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-mediation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Causal Mediation&lt;/h1&gt;
&lt;p&gt;Causal mediation analysis is concerned with a causal graph where a treatment &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; affects an outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and the effect flows at least partially through a mediator &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. Potential outcomes are expressed as &lt;span class=&#34;math inline&#34;&gt;\(Y_{i}(T_{i}, M_{i}(T_{i}))\)&lt;/span&gt;, where the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; depends both on the treatment assignment &lt;span class=&#34;math inline&#34;&gt;\(T_{i} = t\)&lt;/span&gt; and the resulting value of the mediator &lt;span class=&#34;math inline&#34;&gt;\(M_{i}(t)\)&lt;/span&gt;, which is itself affected by the treatment. The causal effects are a decomposition of the total (average) treatment effect.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;total treatment effect&lt;/em&gt;: how much total change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is owed to setting the value of &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;? Written as &lt;span class=&#34;math inline&#34;&gt;\(Y(1, M(1)) - Y(0, M(0))\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;causal mediation effect&lt;/em&gt;: how much of the total change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is attributed to &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;’s effect on &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, which also affects &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;? Or, how much change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is owed to the fact that &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; changed, as opposed to not changing? Written as &lt;span class=&#34;math inline&#34;&gt;\(Y(t, M(1)) - Y(t, M(0))\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;direct effect&lt;/em&gt;: how much of the change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is not flowing through &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;? In other words, how would &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; be different even if &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; had no effect on &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;? Written as &lt;span class=&#34;math inline&#34;&gt;\(Y(1, M(t)) - Y(0, M(t))\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Imai et al. present an algorithm to estimate these quantities. We need models to describe how &lt;span class=&#34;math inline&#34;&gt;\(M(T)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y(T, M(T))\)&lt;/span&gt; are generated, but the form of these models does not affect the intuition of the algorithm. It’s like this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Estimate mediator as a function of treatment and pre-treatment covariates: &lt;span class=&#34;math inline&#34;&gt;\(M_{i} = f(T_{i}, X_{i})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Estimate the outcome as a function of the treatment, the observed mediator, and pre-treatment covariates. &lt;span class=&#34;math inline&#34;&gt;\(Y_{i} = g(T_{i}, M_{i}, X_{i})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Using &lt;span class=&#34;math inline&#34;&gt;\(f()\)&lt;/span&gt;, generate predicted values &lt;span class=&#34;math inline&#34;&gt;\(\hat{M}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Using &lt;span class=&#34;math inline&#34;&gt;\(g()\)&lt;/span&gt;, predicted values &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; for all potential outcomes &lt;span class=&#34;math inline&#34;&gt;\(y(t, M(t&amp;#39;))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Use the appropriate &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; values to calculate average total, direct, and mediation effects.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;doing-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Doing it&lt;/h1&gt;
&lt;p&gt;Imai et al. demonstrate their method using (in part) an experimental study by &lt;a href=&#34;https://www.jstor.org/stable/25193860?seq=1#metadata_info_tab_contents&#34;&gt;Brader, Valentino, and Suhay 2008&lt;/a&gt; on the way news stories affect immigration attitudes through specific emotional mechanisms. Let’s do the outcome where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; represents a participant’s decision to send an anti-immigrant message to their Congressperson (&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), which is affected by a cue in the story about a hypothetical immigrant’s ethnicity (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;), and moderated by the emotion of anxiety (&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;First, Imai et al. use an OLS model to predict respondent’s anxiety in response to treatment, with pre-treatment covariates &lt;span class=&#34;math inline&#34;&gt;\(X_{i}\)&lt;/span&gt; and coefficients &lt;span class=&#34;math inline&#34;&gt;\(\zeta_{1}\)&lt;/span&gt;. Parameters are subscripted &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; for the “first stage” of the estimation.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  M_{i} &amp;amp;= \alpha_{1} + T_{i}\beta_{1} + X_{i}\zeta{1} + \epsilon_{i}
\end{align}\]&lt;/span&gt;
They then use a probit model to estimate the outcome variable, the “second stage” (subscripted 2). This model includes the mediator with coefficient &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p(Y_{i} = 1) &amp;amp;= \Phi\left(\alpha_{2} + T_{i}\beta_{2} + M_{i}\gamma + X_{i}\zeta_{2}\right)
\end{align}\]&lt;/span&gt;
You should be able to code to implement this routine in R &lt;a href=&#34;https://github.com/mikedecr/site-leavit/blob/master/static/code-blogs/R/bayes-mediation.R&#34;&gt;here&lt;/a&gt;, which calls &lt;a href=&#34;https://github.com/mikedecr/site-leavit/blob/master/static/code-blogs/stan/mediation-bvs.stan&#34;&gt;this Stan file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Stan, it is easy to generate posterior quantities of interest in the &lt;code&gt;generated quantities&lt;/code&gt; block of a Stan file. For example, generating posterior predictions for mediator values at &lt;span class=&#34;math inline&#34;&gt;\(T \in \{0, 1\}\)&lt;/span&gt; is as easy as…&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m0 = alpha_m + (0 * beta_m) + (X * zeta_m);
m1 = alpha_m + (1 * beta_m) + (X * zeta_m);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because &lt;code&gt;alpha_m&lt;/code&gt;, &lt;code&gt;beta_m&lt;/code&gt;, and &lt;code&gt;zeta_m&lt;/code&gt; are all uncertain parameters, what we are actually doing is generating &lt;code&gt;m0&lt;/code&gt; and &lt;code&gt;m1&lt;/code&gt; in each iteration of the sampler, thus creating a distribution of predicted mediator values. In the integral notation from above, what we’re actually doing is generating a distribution &lt;span class=&#34;math inline&#34;&gt;\(p\left(M(t)\right)\)&lt;/span&gt; by marginalizing over all of the parameters (except for the error term, which is presumably fixed in the counterfactual case).
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p\left(\tilde{M}(t)\right) &amp;amp;= \int p\left(\tilde{M}(t) \mid \alpha_{1}, \beta_{1}, \zeta_{1}\right) p(\alpha_1, \beta_1, \zeta_1 \mid M)d\alpha_1 d\beta_1 d\zeta_1
\end{align}\]&lt;/span&gt;
Hopefully I haven’t messed up the integral.&lt;/p&gt;
&lt;p&gt;Posterior predictions for new potential outcome observation &lt;span class=&#34;math inline&#34;&gt;\(\tilde{Y}(t, M(t&amp;#39;))\)&lt;/span&gt; would be…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p\left(\tilde{Y}(t, m(t&amp;#39;))\right) &amp;amp;= \int p\left(\tilde{Y}(t, M(t&amp;#39;)) \mid \alpha_2, \beta_2, \tilde{M}(t&amp;#39;), \gamma, \zeta_2 \right) \times \\[6pt]&amp;amp;\qquad p\left(\alpha_2, \beta_2, \tilde{M}(t&amp;#39;), \gamma, \zeta_2 \mid Y\right) d\alpha_2 d\beta_2 d\tilde{M}(t) d\gamma d\zeta_2.
\end{align}\]&lt;/span&gt;
This expression is also marginalizing over the simulated mediator value &lt;span class=&#34;math inline&#34;&gt;\(M(t&amp;#39;)\)&lt;/span&gt;. Because the simulated mediator is a function of random variables, it itself is also a random variable with a probability distribution.&lt;/p&gt;
&lt;p&gt;In order to get total, direct, and mediation effects, we calculate each comparison of potential outcomes using the posterior predictive draws, and then average over each observation in the data. Here are the posterior samples for each treatment effect component.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-19_bayesian-causal-mediation_files/figure-html/post-hist-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is a comparison to the &lt;code&gt;{mediation}&lt;/code&gt; package by the Imai et al. team. We can see that, because the posterior distributions for the ACMEs are not symmetrical, there is some difference between the &lt;code&gt;{mediation}&lt;/code&gt; estimates (which come from an maximum likelihood model) and the Bayesian estimate, which is a posterior mean.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-19_bayesian-causal-mediation_files/figure-html/graph-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-things-to-think-about&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Other things to think about&lt;/h1&gt;
&lt;p&gt;Imai et al. propose a sensitivity analysis to measure “how much” post-treatment confounding among mediators would be enough to change your inference about causal mediation effects. While I won’t do this now, it would be possible to specify a prior on the sensitivity parameter. Such a move would let the researcher evaluate the mediation effect &lt;em&gt;marginal&lt;/em&gt; of a distribution of potential confounding, rather than merely conditional on one fixed level of confounding. This would let us make a probabilistic statement about the threat of confounding rather than a hypothetical statement. It’s of course subject to the prior, but most researchers substantively interpret their results assuming that confounding is zero, so we can think about the prior as actually relaxing an assumption of zero confounding rather than “adding a new assumption.”&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;
There are a few examples of it in political science, but the Bayesian component is used mostly for computation (MCMC) rather than for the Bayesian ideas themselves. Meanwhile Bayes-for-its-own-sake seems far more prevalent in fields like psychology, epidemiology, and biostatistics.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;
Ugh, caveating. It would be possible to represent identification assumptions as special cases of prior distributions, where the parameters of the prior can be manipulated to “relax” the assumption. For example, unconfoundedness or exclusion restrictions imply a model that contains additional covariates that each have priors that stack all probability density at exactly zero. This exercise is actually very similar to the specification of the “sensitivity parameter” in the Imai et al. mediation analysis routine.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Unrealistic Priors and the &#34;Illusion of Learning from Observational Research&#34;
</title>
      <link>/post/ggk-flat-priors/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/ggk-flat-priors/</guid>
      <description>


&lt;p&gt;I’ve been reading about Bayesian causal inference for a paper I’m hoping to write, and this has led me to dig into the work by &lt;a href=&#34;https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=RDueAgAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PA9&amp;amp;dq=gerber+green+kaplan&amp;amp;ots=0I3f6Jfn33&amp;amp;sig=wVKOfRF39mkiT4vnMe8YecMMpeY#v=onepage&amp;amp;q=gerber%20green%20kaplan&amp;amp;f=false&#34;&gt;Gerber, Green, and Kaplan&lt;/a&gt; (hereafter “GGK”) about the “Illusion of Learning from Observational Research.” In it, they put forth a model to describe how much you “update” your information about causal effects from experimental vs. observational research.&lt;/p&gt;
&lt;p&gt;The intuition of the model? Suppose that we want to learn about some true causal effect &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. When we conduct a study, our estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{T}\)&lt;/span&gt; reflects the true effect &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, plus bias &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, plus error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \hat{T} &amp;amp;= \tau + \beta + \epsilon,
\end{align}\]&lt;/span&gt;
What we want to do is update our information about &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. Under what conditions do we learn a lot about it?&lt;/p&gt;
&lt;p&gt;Suppose we collect a ton of data, so the variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat{T}\)&lt;/span&gt; shrinks to be very small. This is similar to shrinking &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; toward zero, so we’re pretty certain what &lt;span class=&#34;math inline&#34;&gt;\(\tau + \beta\)&lt;/span&gt; is, but we’ve only identified their sum. We don’t know how much of &lt;span class=&#34;math inline&#34;&gt;\(\hat{T}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and how much is &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the thrust of their argument. If we want to update our priors about &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; specifically, we need a study design where we have clear priors about &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. If our study is experimental, we can assume &lt;em&gt;a priori&lt;/em&gt; that the size of the bias &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is either zero or very small. This way, learning about &lt;span class=&#34;math inline&#34;&gt;\(\hat{T}\)&lt;/span&gt; allows us to learn a lot about &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. When we are in an observational study, we don’t know as much about the size of the bias &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and the data don’t allow us to update those priors independently of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. As a result, the data can’t tell us as much about &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, only &lt;span class=&#34;math inline&#34;&gt;\(\tau + \beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This model makes a lot of sense. When you have vague priors about the bias in a study, you don’t know how to interpret its findings. Fair!&lt;/p&gt;
&lt;div id=&#34;but&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;But…&lt;/h1&gt;
&lt;p&gt;The namesake of GGK’s theoretical model, the &lt;strong&gt;Illusion of Learning from Observational Research&lt;/strong&gt;, is the model’s result where we have &lt;em&gt;flat priors&lt;/em&gt; about the bias &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Under a flat prior, we learn &lt;em&gt;nothing&lt;/em&gt; from having conducted an observational study because the bias could be &lt;em&gt;anything&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What I want to assert is that the “flat priors” result is a degenerate case. It does not accurately characterize observational research, and it never really occurs in real life. As a result, we need to be careful interpreting the model to avoid overstating what it actually says about real-world observational research.&lt;/p&gt;
&lt;p&gt;Suppose we conduct an observational study and detect an effect of size &lt;span class=&#34;math inline&#34;&gt;\(1.0\)&lt;/span&gt;. If we are concerned about uncorrected bias in the study design, practically speaking we are mainly concerned with the possibility that the effect is &lt;em&gt;over-estimated&lt;/em&gt;—unobserved confounders drive self-selection into treatment, inflating the causal effect estimate. This would give us a prior that the true effect most likely falls somewhere between 0 and the observational effect size. Meaning, we actually have pretty specific priors about the bias. Would we put much probability on the possibility that the true effect is &lt;em&gt;double&lt;/em&gt; the estimated effect, or greater? Probably not, since most of the time we are worried about self-selection into treatment. Do we think that the true effect is just as big as the estimated effect, but in the exact opposite direction? Again, probably not, especially if we have theorized carefully about our expectations for the study.&lt;/p&gt;
&lt;p&gt;We can see it better by rearranging terms. Setting &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; for a moment…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \beta &amp;amp;= \hat{T} - \tau
\end{align}\]&lt;/span&gt;
The bias &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the difference between the true and observed effects, so our prior for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the difference in our priors for the true and estimated effects. If we have some expectation about the true and estimated effects—and we normally do…—then we have reasonably clear priors about the size of the bias in observational studies.&lt;/p&gt;
&lt;p&gt;What does a Bayesian say? Our priors aren’t flat, but so the “Illusion” of learning in observational studies is an overstatement (or even “sleight of hand”). I worry how many people have read the original GGK piece and come away with the impression that flat priors about &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a fair or accurate representation of observational research. I don’t mean this as a defense of biased research—rather, it is a criticism of flat priors. I don’t know if GGK intended for the flat priors case to be interpreted as “realistic”—on the one hand, the flat-priors result is the namesake of the piece, but their numerical example uses a non-flat prior for the bias term—but if a reader isn’t already thinking hard about their priors, then it’s easy to see how they might not catch this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;zooming-out&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Zooming out&lt;/h1&gt;
&lt;p&gt;There are a few mid-level lessons we could reinforce by thinking about our priors about bias in observational studies.&lt;/p&gt;
&lt;p&gt;First, there’s never a bad time to remember that (improper) flat priors are unrealistic. Inferences under flat priors can look like inferences under informed priors if the data are strong enough, but we should worry about any exercise where some theoretical result &lt;em&gt;necessarily depends&lt;/em&gt; on an assumption of flat priors. In thought experiments and in real data analysis, you can always do better than a flat prior.&lt;/p&gt;
&lt;p&gt;Relatedly, Bayesians are keen to highlight areas where informed priors provide important stability to some result that would have looked like nonsense under flat priors. This is one of those cases. Flat priors lead you in an unstable direction assessing the information conveyed by research. It’s only in a case where you have more informed priors about the terms in the GGK model where the results conform to how we actually think about research findings.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lastly, I am increasingly preoccupied by the way Bayes’ theorem is routinely used in theoretical models to convey important intuitions about causal inference and yet there is so little formal incorporation of Bayesian priors in applied data analysis of credible research designs (in political science at least; other fields mix these things much more). I’m trying to write a paper about doing applied Bayesian analysis in causal inference, and I hope the causal inference crowd can be convinced to legalize it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;
Furthermore, it is worth emphasizing that flat priors are only flat &lt;em&gt;with respect to a likelihood&lt;/em&gt;. As soon as you have a quantity that is a function of multiple parameters, the prior for this resulting quantity will &lt;em&gt;not&lt;/em&gt; be flat even if the priors for each parameter are flat. If you have ever demonstrated the Central Limit Theorem by summing a bunch of uniform random variables that results in a bell curve, you have already seen this in action!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Visualization of Partial Effects in Multiple Regression</title>
      <link>/post/viz-partials/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/viz-partials/</guid>
      <description>
&lt;style type=&#34;text/css&#34;&gt;
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
&lt;/style&gt;


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;A few days ago, &lt;a href=&#34;https://www.andrewheiss.com/&#34;&gt;Andrew Heiss&lt;/a&gt; was &lt;a href=&#34;https://twitter.com/andrewheiss/status/1052232993723494400?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1052232993723494400&amp;amp;ref_url=http%3A%2F%2F127.0.0.1%3A4321%2F2018%2F2018-10-19-partialling-out%2F&#34;&gt;looking&lt;/a&gt; for a way to visualize multiple regression with an emphasis on one predictor, without 3(+)-dimensional plots. He works through a method and posts this cool animation, which shows the changing relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; when adding controls, superimposed over the scatterplot of the raw data. (He credits &lt;a href=&#34;https://twitter.com/petemohanty&#34;&gt;Pete Mohanty&lt;/a&gt; with the shifting abline idea.)&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Helpful animated &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; showing what happens to the slope of one coefficient in a model when controlling for other variables in multiple regression&lt;br&gt;&lt;br&gt;(&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; code: &lt;a href=&#34;https://t.co/yhVLj325Oh&#34;&gt;https://t.co/yhVLj325Oh&lt;/a&gt;) &lt;a href=&#34;https://t.co/2foYfXDo28&#34;&gt;pic.twitter.com/2foYfXDo28&lt;/a&gt;
&lt;/p&gt;
— 🎃 Andrew Heiss, scary PhD 🦇 (&lt;span class=&#34;citation&#34;&gt;@andrewheiss&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/andrewheiss/status/1052978108255498240?ref_src=twsrc%5Etfw&#34;&gt;October 18, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;This is cool, but based on Andrew’s initial question, I had something a little different come to mind. I thought we’d be seeing the impact of the regression in both the regression line &lt;em&gt;and&lt;/em&gt; in the data. So I tried to make that (starting with &lt;a href=&#34;https://t.co/yhVLj325Oh&#34;&gt;his code&lt;/a&gt;)…&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Ok fixed. &lt;br&gt;&lt;br&gt;In this fig, y is (beta * humidity), plus the regression residual. This is equivalent to starting with the fully estimated regression and subtracting out terms for every other covariate &lt;a href=&#34;https://t.co/fLs4WxHTaK&#34;&gt;pic.twitter.com/fLs4WxHTaK&lt;/a&gt;
&lt;/p&gt;
— Michael DeCrescenzo (&lt;span class=&#34;citation&#34;&gt;@mikedecr&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/mikedecr/status/1053028075170975744?ref_src=twsrc%5Etfw&#34;&gt;October 18, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;which he &lt;a href=&#34;https://twitter.com/andrewheiss/status/1053031575770718208&#34;&gt;liked&lt;/a&gt; and asked to see the code for.&lt;/p&gt;
&lt;p&gt;So I will deliver. &lt;a href=&#34;https://gist.github.com/mikedecr/f6ffdb716d62af32e701f95231f00bee&#34;&gt;Here&lt;/a&gt; is a gist containing an example, and below is some explanation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intuition&lt;/h1&gt;
&lt;p&gt;Some math will help. Let’s start by writing the regression equation to suit the task at hand: although we include multiple predictors, we only want to highlight one of them, putting the other predictors into a black box “vector of controls.” Andrew’s example uses Dark Sky data on weather in Provo, UT, highlighting the relationship between humidity and a daily temperature high for each day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathit{HighTemp}_{i} &amp;amp;= \alpha + \beta\left(\mathit{Humidity}_{i}\right) + \mathbf{x}_{i}^{T}\gamma + \varepsilon_{i}
\end{align}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the constant, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt; is a column-vector of covariate observations for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (everything but humidity), and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a vector of coefficients for all non-humidity predictors.&lt;/p&gt;
&lt;p&gt;Operationally, what we want to do is show how &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; changes with the inclusion of additional controls. Andrew’s example shows this by plotting different regressions overtop the raw data. If we run the code&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; from his &lt;a href=&#34;https://gist.github.com/andrewheiss/5e162c836575721d1dd53ec2af38753c&#34;&gt;Gist&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-partialling-out_files/figure-html/andrew-animate-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The line being plotted starts with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathit{High}}_{i} = {\alpha} + {\beta}(\mathit{Humidity}_{i})\)&lt;/span&gt; and adds additional covariates one at a time. The data remain intact.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variation&lt;/h1&gt;
&lt;p&gt;But let’s say that we wanted to see the effect of controls in the data as well. This is, I think, where the real umph from this kind of visualization would be; after all, we have already told students that including other predictors will affect the line.&lt;/p&gt;
&lt;p&gt;Thinking about the math, this is as easy as doing to the raw data what we’ve already done to the regression line: subtract out the effect of the covariates. That is, purge the effect of other variables from the raw data. Start with the fully specified regression model…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathit{High}_{i} &amp;amp;= \alpha + \beta\left(\mathit{Humidity}_{i}\right) + \mathbf{x}_{i}^{T}\gamma + \varepsilon_{i}
\end{align}\]&lt;/span&gt;
…and then subtract out the influence of variables in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \label{eq:sub} \mathit{High}_{i} - \mathbf{x}^{T}\gamma &amp;amp;= \alpha + \beta\left(\mathit{Humidity}_{i}\right) + \varepsilon_{i}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We still have to decide what to do with the constant. We could…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leave the constant there, which is probably undesirable because the value of the constant reflects the scaling of other covariates.&lt;/li&gt;
&lt;li&gt;Start by setting all covariates equal to their means. This would give us a prediction that is no longer subject to the &lt;em&gt;scaling&lt;/em&gt; of the covariates but the covariates still affect the mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; overall. This works but I think we can make it simpler.&lt;/li&gt;
&lt;li&gt;Subtract the constant along with the covariates. This leaves us with only the predicted partial effect of humidity (plus error). This is what we’ll do, because it zooms in only on the predictor that we care about.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Now we will create the revised gif.&lt;/p&gt;
&lt;p&gt;First we start with the original Heiss data and code.&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;co&#34;&gt;# ---- Heiss code -----------------------&lt;/span&gt;

&lt;span class=&#34;kw&#34;&gt;library&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;magrittr&amp;quot;&lt;/span&gt;)
&lt;span class=&#34;kw&#34;&gt;library&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;tidyverse&amp;quot;&lt;/span&gt;)
&lt;span class=&#34;kw&#34;&gt;library&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;lubridate&amp;quot;&lt;/span&gt;)
&lt;span class=&#34;kw&#34;&gt;library&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;broom&amp;quot;&lt;/span&gt;)
&lt;span class=&#34;kw&#34;&gt;library&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;scales&amp;quot;&lt;/span&gt;)
&lt;span class=&#34;kw&#34;&gt;library&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;gganimate&amp;quot;&lt;/span&gt;)

&lt;span class=&#34;co&#34;&gt;# Load and clean data&lt;/span&gt;
&lt;span class=&#34;co&#34;&gt;# This data comes from Dark Sky&amp;#39;s API&lt;/span&gt;
weather_provo_raw &amp;lt;-&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;read_csv&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;https://andhs.co/provoweather&amp;quot;&lt;/span&gt;)

&lt;span class=&#34;co&#34;&gt;# clean dates and precip&lt;/span&gt;
weather_provo_&lt;span class=&#34;dv&#34;&gt;2017&lt;/span&gt; &amp;lt;-&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;weather_provo_raw &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;month =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;month&lt;/span&gt;(date, &lt;span class=&#34;dt&#34;&gt;label =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;abbr =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;FALSE&lt;/span&gt;),
    &lt;span class=&#34;dt&#34;&gt;month_number =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;month&lt;/span&gt;(date, &lt;span class=&#34;dt&#34;&gt;label =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;FALSE&lt;/span&gt;),
    &lt;span class=&#34;dt&#34;&gt;weekday =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;wday&lt;/span&gt;(date, &lt;span class=&#34;dt&#34;&gt;label =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;abbr =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;FALSE&lt;/span&gt;),
    &lt;span class=&#34;dt&#34;&gt;weekday_number =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;wday&lt;/span&gt;(date, &lt;span class=&#34;dt&#34;&gt;label =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;FALSE&lt;/span&gt;),
    &lt;span class=&#34;dt&#34;&gt;precipType =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;ifelse&lt;/span&gt;(&lt;span class=&#34;kw&#34;&gt;is.na&lt;/span&gt;(precipType), &lt;span class=&#34;st&#34;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;, precipType)
  ) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;select&lt;/span&gt;(
    date, month, month_number, weekday, weekday_number,
    sunriseTime, sunsetTime, moonPhase, 
    precipProbability, precipType, temperatureHigh, temperatureLow, dewPoint, 
    humidity, pressure, windSpeed, cloudCover, visibility, uvIndex
  )

&lt;span class=&#34;co&#34;&gt;# keep winter and spring, scale vars&lt;/span&gt;
winter_spring &amp;lt;-&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;weather_provo_&lt;span class=&#34;dv&#34;&gt;2017&lt;/span&gt; &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;filter&lt;/span&gt;(month_number &lt;span class=&#34;op&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;dv&#34;&gt;5&lt;/span&gt;) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;month =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;factor&lt;/span&gt;(month, &lt;span class=&#34;dt&#34;&gt;ordered =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;FALSE&lt;/span&gt;)) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;humidity =&lt;/span&gt; humidity &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;dv&#34;&gt;100&lt;/span&gt;, 
    &lt;span class=&#34;dt&#34;&gt;cloudCover =&lt;/span&gt; cloudCover &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;dv&#34;&gt;100&lt;/span&gt;, 
    &lt;span class=&#34;dt&#34;&gt;precipProbability =&lt;/span&gt; precipProbability &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;dv&#34;&gt;100&lt;/span&gt;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We combine several model formulas into a data frame and estimate each regression using &lt;code&gt;purrr::map()&lt;/code&gt;. We’ve added the results from &lt;code&gt;broom::augment()&lt;/code&gt; because we want the residuals from each model to create the “noise” in the data for the graphic.&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;co&#34;&gt;# ---- mike decrescenzo modifications begin -----------------------&lt;/span&gt;

&lt;span class=&#34;co&#34;&gt;# Run all these models in one data frame (purrr::map)&lt;/span&gt;
&lt;span class=&#34;co&#34;&gt;# add the data as a list column because we&amp;#39;ll want it later&lt;/span&gt;
models &amp;lt;-&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;tribble&lt;/span&gt;(
    &lt;span class=&#34;op&#34;&gt;~&lt;/span&gt;formula,
    &lt;span class=&#34;st&#34;&gt;&amp;quot;temperatureHigh ~ humidity&amp;quot;&lt;/span&gt;,
    &lt;span class=&#34;st&#34;&gt;&amp;quot;temperatureHigh ~ humidity + windSpeed&amp;quot;&lt;/span&gt;,
    &lt;span class=&#34;st&#34;&gt;&amp;quot;temperatureHigh ~ humidity + windSpeed + cloudCover&amp;quot;&lt;/span&gt;,
    &lt;span class=&#34;st&#34;&gt;&amp;quot;temperatureHigh ~ humidity + windSpeed + cloudCover + precipProbability&amp;quot;&lt;/span&gt;,
    &lt;span class=&#34;st&#34;&gt;&amp;quot;temperatureHigh ~ humidity + windSpeed + cloudCover + precipProbability + visibility&amp;quot;&lt;/span&gt;) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;co&#34;&gt;# data in a list column&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;spring_data =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;list&lt;/span&gt;(winter_spring)) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;co&#34;&gt;# Run a model in each row&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;model =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;map2&lt;/span&gt;(formula, spring_data, &lt;span class=&#34;op&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;lm&lt;/span&gt;(.x, &lt;span class=&#34;dt&#34;&gt;data =&lt;/span&gt; .y))) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;co&#34;&gt;# Extract model elements&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;model_tidy =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;map&lt;/span&gt;(model, tidy, &lt;span class=&#34;dt&#34;&gt;conf.int =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;), 
    &lt;span class=&#34;dt&#34;&gt;model_glance =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;map&lt;/span&gt;(model, glance), 
    &lt;span class=&#34;dt&#34;&gt;model_fits =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;map&lt;/span&gt;(model, augment)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We calculate the impact of humidity on the high temperature by extracting the humidity coefficient from each model and multiplying it by the raw humidity data (which comes in the &lt;code&gt;augment&lt;/code&gt; results). We will lazily refer to this as the humidity’s “partial prediction” of temperature (thanks to &lt;a href=&#34;https://twitter.com/EvaMaeRey&#34;&gt;Gina Reynolds&lt;/a&gt; for feedback on what this should be called). As a bonus, we will also save the upper and lower bounds of the humidity beta confidence interval.&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;co&#34;&gt;# compute the partial effect of humidity: beta * humidity&lt;/span&gt;
model_partials &amp;lt;-&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;models &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;co&#34;&gt;# get the humidity beta and bounds&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;humidity_beta =&lt;/span&gt; 
      &lt;span class=&#34;kw&#34;&gt;map&lt;/span&gt;(model_tidy, &lt;span class=&#34;op&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;filter&lt;/span&gt;(.x, term &lt;span class=&#34;op&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &amp;quot;humidity&amp;quot;&lt;/span&gt;)&lt;span class=&#34;op&#34;&gt;$&lt;/span&gt;estimate) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;                        &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;as.numeric&lt;/span&gt;(),
    &lt;span class=&#34;dt&#34;&gt;beta_low =&lt;/span&gt; 
      &lt;span class=&#34;kw&#34;&gt;map&lt;/span&gt;(model_tidy, &lt;span class=&#34;op&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;filter&lt;/span&gt;(.x, term &lt;span class=&#34;op&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &amp;quot;humidity&amp;quot;&lt;/span&gt;)&lt;span class=&#34;op&#34;&gt;$&lt;/span&gt;conf.low) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;                        &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;as.numeric&lt;/span&gt;(),
    &lt;span class=&#34;dt&#34;&gt;beta_high =&lt;/span&gt; 
      &lt;span class=&#34;kw&#34;&gt;map&lt;/span&gt;(model_tidy, &lt;span class=&#34;op&#34;&gt;~&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;filter&lt;/span&gt;(.x, term &lt;span class=&#34;op&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &amp;quot;humidity&amp;quot;&lt;/span&gt;)&lt;span class=&#34;op&#34;&gt;$&lt;/span&gt;conf.high) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;                        &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;as.numeric&lt;/span&gt;()
  ) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;co&#34;&gt;# calculate partial effect of humidity and keep the residual&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;unnest&lt;/span&gt;(model_fits) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;mutate&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;partial =&lt;/span&gt; humidity &lt;span class=&#34;op&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;humidity_beta) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;select&lt;/span&gt;(formula, humidity, &lt;span class=&#34;kw&#34;&gt;contains&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;beta&amp;quot;&lt;/span&gt;), partial, .resid) 

&lt;span class=&#34;co&#34;&gt;# get the beta for label plotting&lt;/span&gt;
model_beta &amp;lt;-&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;model_partials &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;select&lt;/span&gt;(formula, &lt;span class=&#34;kw&#34;&gt;contains&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;beta&amp;quot;&lt;/span&gt;)) &lt;span class=&#34;op&#34;&gt;%&amp;gt;%&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;distinct&lt;/span&gt;() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create the figure. The horizontal axis is the raw humidity data. The vertical axis is the humidity effect (&lt;span class=&#34;math inline&#34;&gt;\(\beta \times \mathit{Humidity}_{i}\)&lt;/span&gt;) plus the regression residual &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt;. The regression line is simply &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathit{High}}_{i} = \hat{\beta}\mathit{Humidity}_{i}\)&lt;/span&gt; with a constant of zero. That is, on a given day, a humidity level of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; exerts a negative impact on temperature amounting to &lt;span class=&#34;math inline&#34;&gt;\(\beta z\)&lt;/span&gt;, setting other factors aside. Conveniently, we don’t have to manually subtract the other covariates because we already know how to calculate the vertical axis using the partial effect and the residual (thanks to the math above).&lt;/p&gt;
&lt;pre class=&#34;sourceCode r&#34;&gt;&lt;code class=&#34;sourceCode r&#34;&gt;&lt;span class=&#34;co&#34;&gt;# animate &lt;/span&gt;
&lt;span class=&#34;kw&#34;&gt;ggplot&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;data =&lt;/span&gt; model_partials, &lt;span class=&#34;kw&#34;&gt;aes&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;x =&lt;/span&gt; humidity, &lt;span class=&#34;dt&#34;&gt;y =&lt;/span&gt; partial &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;st&#34;&gt; &lt;/span&gt;.resid)) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;geom_point&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;color =&lt;/span&gt; &lt;span class=&#34;st&#34;&gt;&amp;quot;gray&amp;quot;&lt;/span&gt;) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;geom_abline&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;data =&lt;/span&gt; model_beta, 
    &lt;span class=&#34;kw&#34;&gt;aes&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;intercept =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;slope =&lt;/span&gt; humidity_beta, &lt;span class=&#34;dt&#34;&gt;group =&lt;/span&gt; formula)
  ) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;geom_abline&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;data =&lt;/span&gt; model_beta, 
    &lt;span class=&#34;kw&#34;&gt;aes&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;intercept =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;slope =&lt;/span&gt; beta_low, &lt;span class=&#34;dt&#34;&gt;group =&lt;/span&gt; formula), 
    &lt;span class=&#34;dt&#34;&gt;linetype =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;
  ) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;geom_abline&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;data =&lt;/span&gt; model_beta, 
    &lt;span class=&#34;kw&#34;&gt;aes&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;intercept =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;0&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;slope =&lt;/span&gt; beta_high, &lt;span class=&#34;dt&#34;&gt;group =&lt;/span&gt; formula), 
    &lt;span class=&#34;dt&#34;&gt;linetype =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;
    ) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;theme_minimal&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;base_family =&lt;/span&gt; &lt;span class=&#34;st&#34;&gt;&amp;quot;Fira Sans&amp;quot;&lt;/span&gt;) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;geom_label&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;data =&lt;/span&gt; model_beta, 
    &lt;span class=&#34;kw&#34;&gt;aes&lt;/span&gt;(&lt;span class=&#34;dt&#34;&gt;x =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;35&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;y =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;-70&lt;/span&gt;, 
        &lt;span class=&#34;dt&#34;&gt;label =&lt;/span&gt; &lt;span class=&#34;kw&#34;&gt;paste0&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;quot;beta: &amp;quot;&lt;/span&gt;, &lt;span class=&#34;kw&#34;&gt;round&lt;/span&gt;(humidity_beta, &lt;span class=&#34;dv&#34;&gt;3&lt;/span&gt;)), 
        &lt;span class=&#34;dt&#34;&gt;group =&lt;/span&gt; formula), 
    &lt;span class=&#34;dt&#34;&gt;parse =&lt;/span&gt; &lt;span class=&#34;ot&#34;&gt;TRUE&lt;/span&gt;, 
    &lt;span class=&#34;dt&#34;&gt;family =&lt;/span&gt; &lt;span class=&#34;st&#34;&gt;&amp;quot;Fira Sans&amp;quot;&lt;/span&gt;, 
    &lt;span class=&#34;dt&#34;&gt;size =&lt;/span&gt; &lt;span class=&#34;dv&#34;&gt;4&lt;/span&gt;) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;labs&lt;/span&gt;(
    &lt;span class=&#34;dt&#34;&gt;x =&lt;/span&gt; &lt;span class=&#34;st&#34;&gt;&amp;quot;Humidity&amp;quot;&lt;/span&gt;, 
    &lt;span class=&#34;dt&#34;&gt;y =&lt;/span&gt; &lt;span class=&#34;st&#34;&gt;&amp;quot;Partial Predicted High Temperature (plus residual, °F)&amp;quot;&lt;/span&gt;, 
    &lt;span class=&#34;dt&#34;&gt;subtitle =&lt;/span&gt; &lt;span class=&#34;st&#34;&gt;&amp;quot;{closest_state}&amp;quot;&lt;/span&gt;
  ) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;transition_states&lt;/span&gt;(formula, &lt;span class=&#34;dt&#34;&gt;transition_length =&lt;/span&gt; &lt;span class=&#34;fl&#34;&gt;0.25&lt;/span&gt;, &lt;span class=&#34;dt&#34;&gt;state_length =&lt;/span&gt; &lt;span class=&#34;fl&#34;&gt;0.5&lt;/span&gt;) &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;enter_fade&lt;/span&gt;() &lt;span class=&#34;op&#34;&gt;+&lt;/span&gt;
&lt;span class=&#34;st&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;kw&#34;&gt;ease_aes&lt;/span&gt;(&lt;span class=&#34;st&#34;&gt;&amp;#39;sine-in-out&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-partialling-out_files/figure-html/animated-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Two notes about the confidence interval&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The confidence intervals here don’t look like the ordinary hourglass-shaped intervals in linear regression. This is because the hourglass shape comes from uncertainty in both the constant and coefficients. However, the constant has been subtracted out of these predictions, so uncertainty in this visualization only reflects uncertainty in the humidity effect.&lt;/li&gt;
&lt;li&gt;I would show confidence intervals with &lt;code&gt;geom_ribbon()&lt;/code&gt;, except I can’t get ribbons to animate because of some weird stuff that’s interfering with &lt;a href=&#34;https://github.com/thomasp85/transformr&#34;&gt;transformr&lt;/a&gt; during animation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;thats-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;That’s it&lt;/h1&gt;
&lt;p&gt;I don’t have comments enabled on the website but get at me on &lt;a href=&#34;https://twitter.com/mikedecr&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Maybe it’s a dev version thing, but &lt;code&gt;scales::degree_format()&lt;/code&gt; doesn’t work for me, so I removed it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

---
title: "Lecture 1: Data Manipulation"
description: "Creating, modifying, shaping, and piping data"
author: "Michael DeCrescenzo"
date: '2018-02-09'
slug: "811-data"
categories: ["R", "ps811", "Teaching"]
tags: []
---

# Schedule:

Read this before our first R lecture.


# How to follow along

A script file walking through some of these commands is available [here](https://uwmadison.box.com/s/xqr5frqtwbtlv342fg7zn1ts9va2ei2t).


# Objectives

The goal of this lesson is to simulate some data manipulation for a research project. This requires...

- setting up the project on your computer
- Get data onto the computer (the [ANES](http://electionstudies.org/studypages/anes_timeseries_cdf/anes_timeseries_cdf.htm) should already be downloaded!)
- Load the data into R
- Cleaning and modifying the data
- Doing some simple calculations


Future lessons pick up where this one leaves off, so you must complete this lesson!. We will be [making graphics](811/811-graphics) and doing [statistical analysis](811/811-analysis) that depend on the changes we make in this document.



<!-- # How to read this document -->

<!-- I have also uploaded an R file containing the commands from this document. You can use this other file to follow along with the code on this webpage. You may also benefit from transcribing the code from this page into your own script file, as a way to practice writing the code yourself. -->


# Setting up a project

Each projects should have a dedicated folder on your computer. These folders should be internally organized: separate folders for data, R scripts, writing, other documentation, and so on. 

Here is an example of one of my project folders.

<center>
  <br>
  <img src="img/dir.png" alt="project-directory" style="width: 90%;"/>
</center>
<br>


For this class, set up your class folder like this:

- You should already have a dedicated `ps811` folder
- Inside of `ps811`, create an `R` folder for all of your R materials.
- Inside of `R`, create folders for `data`, `lessons`, and `exercises`. Put all data in the `data` folder, and put any R files corresponding to online lessons and lecture in the `lessons` folder. 
- (Exercise files will go in the `exercises` folder). 



## Tips for project folders

- When you name your folders and files, use hyphens or underscores to separate words, not spaces. Something like `811-data-lesson.R` is better than `811 data lesson.R`. 
- Use short folder and file names names when you can. This makes your life easier in the long run (e.g. with Git, which I recommend learning).
- Avoid setting up the folders in ways that would require you to navigate *up* the directory tree. See below for more about what I mean.



# Directories

Just as you can look around your computer in the file browser ("Finder" on Mac and "Explorer" on Windows), R looks around your computer as well, but it needs your help knowing where to look.

Find the current working directory with `getwd()`. 

```{r}
# "get working directory" 
# prints the current directory pathway (here is mine)
getwd()
```

To find your project folder, *set your working directory* to the appropriate path (the `ps811/R/lessons` folder).

```{r, eval = FALSE}
# setwd() = "set working directory" 
# changes the directory to the specified file path
# the "~" is a shortcut that means "the top of my user profile" 
setwd("~/pathway/to/ps811/R/lessons")

# confirm directory location
getwd()
```

As with many R functions, you know `setwd()` worked when it gives you no feedback whatsoever. 

You should always set your directory to the *top* of the project folder (also called the "project root" of the project). Do *not* set it all the way into the `data/` folder. This is because it is always easier to navigate *down* into the data folder than it is to navigate *up* back into the project folder. That's because the keyword for doing up a folder, `..`, is very uninformative. 



# Getting started

Load some packages so they can be used in this R session. These should already be downloaded from the previous lesson. 

```{r}
# package loading is done using library().
# Quotes around the package name aren't necessary 
# but recommended for technical reasons

library("magrittr")
library("tidyverse")
```

When you load `tidyverse`, you should see a warning that it "masks" some names in the `magrittr` and other packages. This means some of the function names in the packages are the same. This is *generally* something you should be watching out for, since your code may not work as intended. It won't be a problem right now, though.


# Read external data

With your directory set at `ps811/R/lessons`, confirm that the ANES data file
are where they should be using `list.files()`. You can look inside folders by adding the folder path as an argument. The ANES data should be in the `data/` folder.

```{r, eval = FALSE}
# this is similar to typing `ls` in your computer's shell / command line
list.files()

# you should see the "data" folder.
# Now look inside the "data" folder; should see the ANES file
list.files("data")
```

R has many functions for "reading" datasets into memory, depending on the file format. They typically follow a format of `read.xyz()` or `read_xyz()`, where `xyz` refers to the file type. 

We want to read a Stata file, which uses the `.dta` file extension. Although many tutorials will recommend using the `foreign` package for Stata files, `foreign` is outdated and can fail with newer Stata files. I usually use the `haven` package.

```{r, eval = FALSE}
# should already be installed
library("haven")

# read_dta("path/to/data-file.dta")
anes <- read_dta("data/anes_timeseries_cdf.dta")
```

This should take a few moments to load because the ANES cumulative file is a big file. Don't worry.

```{r, include = FALSE, cache = TRUE}
anes <- haven::read_dta(here::here("static/data/anes_timeseries_cdf.dta"))
```

You may want to check out the `rio` package for more generic file-reading functions.



# Data frames

Data sets in R are known as "data frames." Data frames are tabular objects where the columns are variables with variable names. This is just like Stata, but what makes R different is that we can have many data frames in R at one time. 

Print the data to see what the table looks like.

```{r, eval = FALSE}
anes
```

Technically speaking, the `anes` object isn't an ordinary data frame. It's a "tibble", which is a modified data frame object, the main differences being that tibbles are often faster and prettier when printed to the console. 

You can always print a full data frame by coercing a tibble to the standard data frame class, but be warned: the results can be ugly if R tries to print a full data frame.

```{r, eval = FALSE}
# as.data.frame() coerces an object to be data.frame class
# (there are many different as.class() functions for coercing data)

# You should run this command to see how it works, but be warned: 
# this is gonna look gnarsty
as.data.frame(anes)
```


You can create your own data frame using `data_frame()`. An example:

```{r}
# create x and y variables,
# then use x and y to create other variables 
#   (inside the same function!)
data_frame(x = 1:3, 
           y = 4:6, 
           z = x + y, 
           abc = z * z)
```




## Learning about data frames

Some functions for learning about your data:

Find the number of rows (observations). In this case, rows are survey respondents.

```{r}
# number of rows
nrow(anes) 
```

Find the number of columns (variables).

```{r}
# number of columns
ncol(anes)
```

Print the top or bottom of a data frame to get a glance of it.

```{r, eval = FALSE}
# top 6 and bottom 6 rows
# can set n != 6 if you desire
head(anes) 
tail(anes)
```

Print a vector of variable names.

```{r, eval = FALSE}
# vector of variable names
names(anes)
```


## Other functions for viewing data

There are two for interacting directly with a data table as if it were a spreadsheet: `View()` and `edit()`. I recommend against using these, because these functions often make R freak out and freeze. The *only time this has ever worked for me* is when I was using Rstudio, which has a handy spread sheet window for datasets. 

If you must edit data as a spread sheet, you may want to save it as a `csv` file and open it in Numbers, Excel, or some other similar program. 


# Variables in a data frame

Here is a funky but *extremely important* R thing. Variables in a dataset cannot be accessed by typing only the variable name. 

Try to print `VCF0004` variable. The codebook (which you should have!) tells us that it's the election cycle variable.

```{r, eval = FALSE}
VCF0004
```

You should get an error. To prevent the error, use `dataset$variable`.

```{r, eval = FALSE}
# this should work
anes$VCF0004
```

The `$` symbol tells R that the variable is *inside* the `anes` dataset. This is necessary because we may have multiple datasets in R with the same variable names, so we have to be specific when we want one particular variable. Yes, this is more complicated than Stata, but it lets us do a lot of cool stuff that Stata makes difficult or tedious.





# DO NOT attach data

Others may have advised you that you can sidestep the `dataset$variable` syntax by attaching data (`attach(dataset)`). *Ignore that advice*. You should not attach data. It does not do what you think it does, and it creates so many more (invisible) problems than it solves. Attaching data is playing with fire.

My policy for this class is that attaching is forbidden. Do not attach data for your homework. Do not attach data for your final project. Do not attach data in a house or with a mouse. If you are used to attaching data, I'm sorry, but you will thank me later.

Luckily for us (and all of the R community), the `tidyverse` package provides many tools that make attaching unnecessary. 



# The Tidyverse

Now, the good stuff. The [Tidyverse](https://www.tidyverse.org/) describes itself as...

> ...an opinionated collection of R packages designed for data science. All packages share an underlying philosophy and common APIs. 

## In short

There are lots of R packages out there. Some of them exist under umbrellas of shared use and interface. You can think about the Tidyverse as an umbrella of similar packages that are designed to have similar interfaces and intuitions.

The "Tidyverse" as a *concept* refers to the collection of packages, including `tidyr`, `dplyr`, and so on. The `tidyverse` package *unto itself* is a relatively recent entity. It is simply a bundling of the most-used Tidyverse tools (not *every* tool, however). 

The [packages in the Tidyverse](https://www.tidyverse.org/packages/) are all designed to clean and manipulate data according to the principle of "tidy data" (described below). Moreover, the tools are designed with a coherent syntax that makes them easy for beginners to learn, easy to understand (when you read the code), easy to integrate into complex analysis, and easy for visualizing data. It is hard for me, as someone who has been using R for years, to describe to newcomers just how much the Tidyverse has improved the world of R in recent years.

## In medium

The Tidyverse is based on a philosophy is based on *tidy data*, which refers to data organization where (1) rows contain cases, (2) columns contain variables, and (3) cells contain variable values. 

Although this sounds simple, there are many data formats that do not fit that pattern. Legislative data, for example, is often represented as a vote matrix with legislators in rows and bills in columns. The cells, in turn, would contain `1`s and `0`s to indicate legislators' Yea and Nay votes, respectively, on each bill. A tidy legislative voting dataset, on the other hand, would have a legislator variable, a bill variable, and a vote variable. Both datasets contain the same information, but the organization of the data allow for different sorts of manipulations. And it so happens that the tidy organization is particularly helpful for doing a lot of powerful stuff with R with very little code. 

For more resources about the Tidyverse (aside from this course), you can visit a webpage devoted to [such resources](https://www.tidyverse.org/learn/), which links you to the [R 4 Data Science](http://r4ds.had.co.nz/) book, a book on [`ggplot2`](https://github.com/hadley/ggplot2-book), and various [cheatsheets](https://www.rstudio.com/resources/cheatsheets/) for the constituent packages of the Tidyverse. I got my start using [this blog post](https://rpubs.com/bradleyboehmke/data_wrangling), which describes the concept of "piping data" (which we will cover later) and two of the most important Tidyverse packages: `dplyr` and `tidyr`. We will cover the main points of the Tidyverse, but I would bookmark these resources for later browsing.

**One last note before *really* jumping in:** It is a conscious decision to focus on the Tidyverse at the expense of other approaches (namely, "base R"). As I wrote in [the online introduction to these R lessons](811/811-intro), this is because we have a limited time to make R fun and accessible, and base R is not ideal for that purpose. That being said, you may encounter base R online and in replication materials for other studies, so you should be open to learning a little bit about it on your own time. But I would encourage against making it your "default" mode of doing R, because it is quite old and inefficient compared to the tools we'll learn in class. Hopefully you'll see how easy the Tidyverse is as we proceed!



# How `tidyverse` functions work

Tidyverse literature refers to its core functions as "verbs." These are functions that take a data frame and modify it. 

When you call one of these verbs, you declare the dataset name in the function, so you don't need to use the `$` to reference a variable (nice!). The function assumes that the variable is located in the declared dataset, and if not, it will check the global R memory for objects of the same name. A generic example:

```{r, eval = FALSE}
# overwrite the old dataset with the results
# declare the dataset as the first argument, then specify any arguments
dataset <- verb_name(dataset, verb_arguments = ...)
```

We will review the following `tidyverse` functions from the `dplyr` package. The `dplyr` tools are designed to manipulate data that is already in a tidy format.

- `rename()`: renaming variables
- `mutate()`: create and recode variables
- `select()`: keep/eliminate variables (columns)
- `filter()`: keep/eliminate observations (rows)
- `summarize()`: collapse/aggregate data (e.g. summary statistics, group means...)
- `group_by()`: create groups out of your data (e.g. for summarizing within group)
- `arrange()`: sorting data
- various `join` functions: merging data

The `tidyr` package is designed to take un-tidy data and make it tidy. We'll cover these functions:

- `gather()`: turn many columns into one column (wide to long)
- `spread()`: turn one column into many columns (long to wide)

Most data manipulation tasks fall under the umbrella of one or more of these functions. This is what makes the Tidyverse so useful: it has a small number of very powerful tools that facilitate nearly all common data munging tasks!


# Renaming

The `rename()` function takes a data frame with one set of names, and it returns a data frame with different names. 

Here, we rename the following variables: 

- election year
- respondents' ratings of the two major US parties on a 7-pt ideological scale (1 "extremely liberal" to 7 "extremely conservative")
- respondents' ratings of themselves on the same scale
- and a 7-pt index of party ID (Strong Democrat, Weak Democrat, Independent leaning Democrat, True Independent, Leaning Republican, and so on)

```{r}
# rename(dataset, 
#        new_name = old_name,
#        new_name2 = old_name2)

anes <- rename(anes, 
               cycle = VCF0004,
               libcon_demparty = VCF0503, 
               libcon_repparty = VCF0504, 
               libcon_self = VCF0803,
               pid7 = VCF0301)
```

Be warned: this command *overwrites the original `anes` dataset* with a new version with different variables names. If you want to keep the original variables, copy the old variable into a new variable with a new name. How would you do that? Well...


# Modifying data

Modify the columns of a dataset with `mutate()`. We can create new variables or modify existing variables (a.k.a. "recoding").

How it works: declare an existing data frame, modify variables within it, and the result is a new data frame with the specified changes.

Let's test it out. Let's look at the ideological self-placement variable.

```{r}
table(anes$libcon_self, exclude = NULL)
```

We only care about the values 1 through 7. We want to recode everything else as missing (`NA`). Here are the two methods I use most often to modify variables.



## The `ifelse()` function

The `ifelse()` function follows the following psueocode intuition:

```
if (this) then {that} 
  else {something else}
```

You may have seen these sorts of if-else statements in other programming languages. In R, we can use the `ifelse()` function to apply an if-else statement to an entire vector (variable).

A dummy example: 

```{r, eval = FALSE}
my_result <- ifelse(condition, A, B)
```

The `ifelse()` function checks a condition in the data. This condition is a logical statement: is something equal to something, greater than something, and so on. If the condition applies (meaning, if the statement evaluates to `TRUE`), the result is `A`, and if the condition does not apply, the result is `B`. 

We will use it within the `mutate()` function. I have to get creative with code indentation to make this example more legible, so bear with me.

```{r}
# translation: 
#   modify anes
#   recode libson_self, the result of ifelse()
#   ifelse(logical test, result if TRUE, result if FALSE)
#   and recode demparty

# Notes: 
#   notice how I can break lines after the <- symbol
#   this is because <- needs a left-side and right-side object
#   if there is no right-side object, R looks for on next line
#   this works for all "binary operators" such as +, -, *, and so on
#   In general, R will look at the next line until a statement is complete
#   so you should be careful to close all "quotes" and (parentheses)
anes <- 
  mutate(anes, 
         libcon_self = ifelse(libcon_self == 0 | 
                                libcon_self >= 8 | 
                                is.na(libcon_self), 
                              NA, 
                              libcon_self), 
         libcon_demparty = ifelse(libcon_demparty %in% 1:7, 
                                  libcon_demparty, 
                                  NA))
 ```

We recode two variables in this `mutate()` call. We use `ifelse()` in slightly different ways. Here is the translation:

- If `libcon_self` is 0 *or* greater than *or* equal to 8 *or* NA, *then* recode to `NA`, *else* recode to the existing value of `libcon_self` (i.e. no change). 
- If `libcon_demparty` is an integer value 1 through 7, keep it the way it is, else recode to `NA`. We'll return to the `%in%` operator at the end of this section.

Read the binary logical operations as follows:

- `A == B`: A is equal to B
- `A > B`: A is greater than to B
- `A < B`: A is less than to B
- `A >= B`: A is greater than or equal to B
- `A <= B`: A is less than or equal to B
- `A | B`: A or B
- `A & B`: A and B
- `A %in% c(B, C, D)`: A is equal to any of the following: `B`, `C`, or `D`. Or, `A` is some element of the set `{B, C, D}`.
- `is.na(A)`: A is `NA`.
- `!is.na(A)`: A is *not* `NA`. 

Heads up: nothing can be *equal to* `NA`. That's because `NA` isn't a value; it's a stand-in for an unknown value. This is why we use the `is.na()` function.

Also: the `%in%` operator is a godsend because...

- `x == (1 | 2 | 3)` doesn't work
- `x == 1 | x == 2 | x == 3` works but is annoying

Instead, it is easier to type `x %in% c(1, 2, 3)` (or `x %in% 1:3` if we want to match adjacent integers). 


## The `case_when()` function

Whenever we have multiple conditions that we want to check in the same function, we could nest several `ifelse()` functions within each other or use a very complicated logical test, but it gets quickly gets ugly and is easy to mess up.

In situations like this, you should use the `case_when()` function, which is a more flexible version of `ifelse()`, but it's a little different. See the comments in the code chunk below for how it works. 

```{r}
# case_when(test ~ result if test is true,
#           test2 ~ result if test2 is true,
#           test3 ~ you get the idea)

# NOTE: all non-matched cases are given an NA by default
# to override this, use the catch-all replacement: TRUE ~ x

# case_when(test ~ result,
#           test2 ~ result2,
#           test3 ~ result3,
#           TRUE ~ result4)

# read as "and everything else should be recoded as result4"

anes <- mutate(anes,
  libcon_repparty = case_when(
                      libcon_repparty == 1 ~ libcon_repparty,
                      libcon_repparty == 2 ~ libcon_repparty,
                      libcon_repparty %in% c(3, 4) ~ libcon_repparty,
                      libcon_repparty %in% (5:7) ~ libcon_repparty),
  pid7 = case_when( (pid7 %in% 1:7) ~ pid7) )
```

You may find it helpful to place the tests in parentheses, as I do in the `pid7` example. It doesn't affect the code, but is sometimes easier to read, especially if the logical test is complicated or lengthy.

The above code, translated:

- If `libcon_repparty` is equal to 1, replace with `libcon_repparty` (i.e. keep the same). If it is 2, keep the same. Same with values 3 and 4, and 5 through 7. The only reason I break these into different conditions is to show you different ways to do this logical matching.
- If the `pid7` variable is an integer value 1 through 7, keep it the same.



## General tips for recoding

Depending on what you're doing, you may not want to overwrite the original data. Instead you might create a new variable that modifies the original.

When you recode new variables, you can compare the new and old variables using the `table()` function.

I try to consolidate all of my data cleaning tasks into as few calls to `mutate()` as possible. This makes it easy to retrace your steps. In fact, I usually devote an entire `.R` file in my project solely to cleaning the original data. Once the data are clean, I save a cleaned dataset, and then load the cleaned dataset in a separate `.R` file for subsequent analyses.


## Variables from other variables

Intuitively, you can create variables from other variables. 

Here, we calculate the ideological distance between one's self placement and their placement of the two parties. We also calculate how far apart they perceive the parties to be. The sign of the result ($+/-$) indicates the ideological direction of the difference. Negative values indicate that respondents find themselves to be more liberal than a party, or that they find the Republican Party to be more liberal than the Democratic Party (which would be weird, but hey, that's survey data for you). 

```{r}
anes <- mutate(anes, 
               dem_distance = libcon_self - libcon_demparty,
               rep_distance = libcon_self - libcon_repparty,
               party_distance = libcon_repparty - libcon_demparty)
```

`mutate()` is great because you can create a variable in one line and use it in another line, without ending the `mutate()` call. 



## Manipulating factors and strings

You should check out the `forcats` and `stringr` packages. They are tools for manipulating factors and character variables (a.k.a. "strings"), respectively, and are loaded when you load the `tidyverse` package. We will revisit these when we discuss graphics in the next lesson, but the functions from these packages that I use the most are (using the `pkg::function` notation)...

- `fct_relevel()`: reorder the levels in a factor
- `fct_recode()`: recoding a factor, but `case_when` works here also
- `str_sub()`: extract a pattern from a string
- `str_split()`: split a string at a certain character 
- `str_detect()`: returns `TRUE` if a string contains a pattern 
- `str_replace()`: replace a pattern in a string with a different pattern 

Check online or investigate the help files to learn about how they're used. We'll see some examples next week when we do graphics.




# Selecting columns

Maybe you don't need all these variables for something. You can select specific variables using the `select()` function.

```{r}
# from the anes dataset, grab only the cycle variable
select(anes, cycle)
```

There are various "select helper" functions that aid us in selecting variables. 

```{r, eval = FALSE}
# select a series of variables in the data frame using `:` 
# grab cycle, and all variables between dem_distance and party_distance
select(anes, cycle, dem_distance:party_distance)

# select cycle and any variable containing "libcon" in the variable name
select(anes, cycle, contains("libcon"))

# drop variables using negative sign `-`
# matches(".") means "all remaining variables"

# drop cycle, keep all remaining variables
select(anes, -cycle, matches("."))
```

Learn more about "select helper functions" in the help file (`?select`).


# Filtering rows (subsetting)

Let's say we only want some of the observations. Use the `filter()` function and a logical test to identify the cases you want.

```{r}
# keep cases from 2012 cycle
filter(anes, cycle == 2012)
```


# Summarizing

`summarize()` will process multiple observations into summary statistics. This is also known as "collapsing" or "aggregating."

Find the mean ideological distance between the two parties (as judged by the respondents).

```{r}
# data contain NAs so we use na.rm = TRUE
summarize(anes, 
          mean_party_distance_na = mean(party_distance, na.rm = TRUE))
```



# Grouping and summarizing

Most of the time you will use `summarize()` in conjunction with `group_by()`, which groups the data by some selection of variables. It doesn't modify the cells in any way; it only implicitly partitions the data for later calculations.

For instance, let's say we want to do find the number of observations within each election year the above calculation but within each election year. The `n()` function, when used inside of `summarize()` or `mutate()`, will find the number of observations.

```{r}
# group by cycle
s <- group_by(anes, cycle)

# summarize the grouped data
summarize(s, 
          n = n(),
          mean_party_distance = mean(party_distance, na.rm = TRUE))
```

A data frame can later be ungrouped with (wait for it...) `ungroup()`.




# Sorting data

Sort data frames using `arrange()`. By default, sorting is done in ascending order. Sort variables in descending order using the `desc()` function within `arrange()`. You can also sort by multiple variables at once. 

```{r}
# keep cycle and party distance variables
# sort by cycle (descending order) and then by party distance
arr <- select(anes, cycle, party_distance) 
arrange(arr, desc(cycle), party_distance)
```

Get a sense for the sorting precedence with this toy example.

```{r}
# create some data to sort
d <- data_frame(x = c(1, 1, 2, 2), 
                y = c(1, 2, 1, 2))
d
# sort by y and then by x (meaning, x within y)
arrange(d, y, x)
```


# Joining (merging) 

Merging is done with `join` functions. 

Because the ANES is so big, this concept will be easier to demonstrate with a toy example. Create two datasets with some overlapping cases and some non-overlapping cases:

```{r}
# cases 2 and 3 appear in both data sets
# cases 1 and 4 exist in one dataset but not the other
(data1 <- data_frame(case = 1:3, 
                     var1 = c("a", "b", "c")))
(data2 <- data_frame(case = 2:4, 
                     var2 = c("x", "y", "z")))
```

We want to put these datasets together into one table, matching data to the appropriate cases.

The `full_join()` function keeps all cases from both datasets. Non-matching cells are filled with `NA` by default, but you can specify replacement values for non-matches if you wish.

```{r}
# join all cases, keep non-matches
full_join(data1, data2, by = "case")
```

You can merge along multiple variables using `by = c("var1", "var2", ...)`. 

`left_join()` keeps all cases from left dataset. If the right dataset can't fill a cell in the left dataset, the result is `NA`. If the right dataset has other cases that aren't present in the left, they disappear.

```{r}
# keep left data in tact, 
# merge only matching cases from right data
left_join(data1, data2, by = "case")
```


`right_join()` is the mirror of `left_join`.

```{r}
right_join(data1, data2, by = "case")
```



`inner_join()` keeps only cases with matches in both datasets.

```{r}
# keep only matching cases, drop everything else
inner_join(data1, data2, by = "case")
```


`anti_join()` is a little funky and different. It keeps only the *unmatched* cases from the left dataset only. This is helpful for diagnosing a problem with an imperfect attempt to join two data frames.

```{r}
# what doesn't match?
anti_join(data1, data2, by = "case")
anti_join(data2, data1, by = "case")
```



# Tabulating

Here are two ways to tabulate data: `table()` and `count()`. `table()` produces a table object; `count()` produces a data frame.

## `table()`

First, `table()`. The `exclude = NULL` argument will force R to print the number of missing values (which it does not do by default).

```{r}
table(anes$libcon_self, exclude = NULL)
```

Turn frequencies into proportions by wrapping a table object with `prop.table()`.

```{r}
prop.table(table(anes$libcon_self, exclude = NULL))
```

You can round these proportions to get more manageable values.

```{r}
# round to 3 decimal positions
round(prop.table(table(anes$libcon_self, exclude = NULL)), 3)
```

Notice how these are nested functions, like $f(g(h(x)))$. This is the kind of flexibility that we like about R.

To make two-way tables, the first variable prints as rows, and the second as columns. 

```{r}
# pid7 as rows, libcon_self as columns
table(anes$pid7, anes$libcon_self, exclude = NULL)
```

By default, `prop.table()` estimates proportions out of the entire table. To estimate proportions within rows or columns, use the `margin` argument (`margin = 1` for the fraction within a row, `margin = 2` for the fraction within a column).

```{r}
# two-way table
tab <- table(anes$pid7, anes$libcon_self, exclude = NULL)

# proportions as fractions of each row
ptab <- prop.table(tab, margin = 1)

# round the result
round(ptab, 3)
```


## `count()`

If you want your table to be organized as a data frame (which you often do want), use `count()`. This is nice for doing further calculations, exporting results (e.g. to ${\mathrm{\LaTeX}}$), and so on. 

```{r}
# from the anes data, count the intersections of cycle and party ID
count(anes, cycle, pid7)
```

This contains the same information as `table(anes\$cycle, anes\$pid7)`, but the result is a "tidy" data frame. 

To get proportions, mutate the resulting table as desired. Here we get the proportion of each partisan identity within each election cycle. Group on the cycle and then dividing each count by the number of individuals in each cycle.

```{r}
# tabulate these variables, party by cycle
(tab <- count(anes, cycle, pid7))

# group by cycle
# find n per cycle
# divide counts by n_in_cycle
# round
mutate(group_by(tab, cycle), 
       n_in_cycle = sum(n, na.rm = TRUE),
       p = n / n_in_cycle,
       p = round(p, 3))
```

The `count()` function also handles sample weights. 

```{r}
# keep only elections since 2000
# tabulate cycle and party
# apply sample weights
count(filter(anes, cycle >= 2000), 
      cycle, pid7, 
      wt = VCF0009z)
```

The "counts" are no longer whole numbers, thanks to the survey weights (some people count as "partial observations" due to sample design).



# Tidyr functions

This concludes today's foray into the `dplyr` family of functions. Now we'll switch to the `tidyr` functions. The main difference is that `dplyr` tends to *change* data while `tidyr` simply moves it around.

We'll talk about "wide" and "long" data. 

- Wide data might be data from multiple time periods, where variables from different time periods are represented as different columns. So we might have different `x` and `y` variables from three different time periods as columns named `x1`, `x2`, `x3`, `y1`, `y2`, `y3`.
- Long data would have the same information as the wide data, but instead of different variables for time periods, we stack the time periods on top of one another into one variable. So we would have variables for `x` and `y` as well as a `time_period` variable to indicate which observations come from which wave. 

When we shape data, what we're really doing is moving data around (also called "reshaping") to make it long (elongating) or wide (widening). 


# Reshape wide to long, with `gather()`

Gathering will take multiple columns and stack the cells into one variable (with an accompanying variable for labeling). 

Here is an example using the ideological distance variables from above. First we have to prep some data so we can see how this works.

```{r}
# keep only certain variables
d <- select(anes, cycle, dem_distance, rep_distance)

# keep certain election years
d <- filter(d, cycle %in% c(2004, 2008, 2012))

# get mean in each year
d <- summarize(group_by(d, cycle), 
               dem_distance = mean(dem_distance, na.rm = TRUE),
               rep_distance = mean(rep_distance, na.rm = TRUE))

# show results
d
```

We'll gather the two distance variables into one variable, with another variable to indicate which party we're contrasting from.

```{r}
# gather(data, resulting key, resulting value, initial varlist)
l <- gather(d, key = party, value = distance, dem_distance, rep_distance)
l
```

Select helper functions also work for selecting which variables to gather.

```{r}
# gather(data, key, value, varlist)
gather(d, key = party, value = distance, contains("distance"))
```


# Reshaping long to wide, with `spread()`

Spreading is the opposite of gathering. It takes a column and unstacks it into several columns. We need a corresponding label variable also, which becomes the variable names. Observe:

```{r}
l

# 'key' variable becomes new variable names. 
# 'value' variable becomes new variable values
spread(l, key = party, value = distance) 
```



# Piping data

This section is *extremely important*.

Now that we have covered some essential tools for wrangling data, let's tie it all together with the concept of ***piping***.

Let's start by identifying the problem. Data processing requires a lot of steps (each represented by the functions we have learned so far). Many of these steps are related. Can we string these operations together in a way that is easy to understand and easy to write?

One (suboptimal) way to sew multiple operations together is with nested functions. Just as we can nest functions in math like $f(g(h(x)))$, we can also do this with R. The problem with this is that the order of operations creates an unintuitive reading experience---we have to read from the inside out. The code becomes ugly and difficult to interpret.

```{r, eval = FALSE}
# this works, and it follows order of operations, but it's annoying
tidy_data <- gather(summarize(group_by(filter(select(dataset, ...), ...), ...), ...), ...)
```

Another (suboptimal) way would be to break up the operation into multiple lines. The problem with this method is that it is verbose and creates a lot of redundancy with object assignment (which can slow down your code with big datasets).

```{r, eval = FALSE}
# each function takes the results from the previous function
# this also works, but requires overwriting the data a lot
# and lots of redundant `d <- ` instances
d <- select(dataset, ...)
d <- filter(d, ...)
d <- group_by(d, ...)
d <- summarize(d, ...)
d <- gather(d, ...)
```

We'll use the *pipe operator* `%>%` to make this process easier. The pipe operator takes a left-hand side object and "pipes" it into a right-hand side function. It sounds trivial, but just wait. 

Here is how it works. We'll use `x` to represent data and `f` to represent functions. 

- By default, `x %>% f()` sets `x` as the first argument in `f`. So `f(x)` is equivalent to `x %>% f()`.
- If `x` is needed elsewhere inside of `f` besides the first argument, we can use `.` to stand-in for `x`. For example, `f(arguments, data = x)` is equivalent to `x %>% f(arguments, data = .)`. 
- If only one argument is needed, you can drop the parentheses on `f()`. So `f(x)` is equivalent to `x %>% f()`, which is equivalent to `x %>% f`. I would recommend you keep the parentheses, however, because it's easier to see which names are associated with data and which names are associated with functions. 

The pipe operator allows you to do multiple dataset operations in a *linear* fashion without creating a ton of intermediary objects. The above processing task could be written as the following "pipe chain."

```{r, eval = FALSE}
# take `dataset` object and pass it to select
# the result from select is passed to filter
# and so on
d <- dataset %>%
  select(...) %>%
  filter(...) %>%
  group_by(...) %>%
  summarize(...) %>%
  gather(...) %>%
  print() 
```

Adding `print()` at the end of the chain will print the results even if you are assigning the results to `d`.

What has the pipe chain done? It has made our code linear and readable, and it makes our workflow more straightforward because we can *think* linearly again. Reading a complex set of operations linearly isn't normally something you can easily do with programming, so we should really appreciate this!

Here's an example using real data. The pipe chain makes it extremely easy to understand exactly what the code is doing. 

```{r}
# start with anes
#   keep certain variables
#   keep certain observations
#   group and summarize
#   gather
#   print result of the chain
l <- anes %>%
  select(cycle, contains("distance")) %>%
  filter(cycle %in% c(2004, 2008, 2012)) %>%
  group_by(cycle) %>%
  summarize(n = n(),
            Democratic = mean(dem_distance, na.rm = TRUE),
            Republican = mean(rep_distance, na.rm = TRUE)) %>%
  ungroup() %>%
  gather(key = party, value = distance, Democratic, Republican) %>%
  print() 
```

We can use pipes to simplify other tasks from earlier in this lesson, like processing a table using proportions and rounding but without all of the nested functions.

```{r}
# create a table
# send table to prop.table to calculate proportions
# send proportions to round() 
table(anes$pid7, anes$libcon_self, exclude = NULL) %>% 
  prop.table(margin = 1) %>%
  round(3)
```

Although we covered piping last, you should not view it as afterthought. The pipe operator will change the way you use R forever. 


# Tips for piping

## Create a keyboard shortcut

If your text editor has the capability, *create a keyboard shortcut for the pipe operator!* I create this kind of thing with Sublime Text all the time. Rstudio can do it as well. Here's what I do:

- `super + .` creates a pipe
- `super + shift + .` creates a pipe and adds a new line
- relatedly, I use `super + shift + ,` to create an assignment operator (`<-`).



## Other helpful pipes

There is one other helpful pipe-like operator that we will talk about: `%$%`. It tells a right-hand function that the variable names in the function come from the left-hand dataset. It doesn't pipe the entire dataset per se; it only says "look here for variable names." The two following commands do the same thing:

```{r}
# look in anes for pid7 variable
table(anes$pid7)

# look in anes for pid7 variable
anes %$% table(pid7)
```

This is useful when you need to reference multiple variables.

```{r}
# notice which pipe I use after 'anes'
# %$% for piping just variable names
# %>% for piping the entire object

anes %$% 
  table(pid7, libcon_self, exclude = NULL) %>% 
  prop.table(margin = 1) %>%
  round(3)
```


# Saving data

You can write or save data from R with many `write.xyz` or `save.xyz` functions. I usually prefer to save data in an R-specific format.

You should save the data from this lesson so we can come back to it next week.

```{r, eval = FALSE}
saveRDS(anes, "data/anes-modified.RDS")
```

```{r, include = FALSE}
saveRDS(anes, here::here("static/data/anes-modified.RDS"))
```

If it's possible that someone using Stata (or some other software) might be using your data, you might save in a more accessible format such as `.csv`.

```{r, eval = FALSE}
write_csv(anes, "data/anes-modified.csv")
```

R can read and save to a multitude of file types, including `.dta` for Stata. R could put Stat/Transfer out of business if more people knew about it. 

Some packages provide "swiss-army-knife" data input/output services, such as the `import()` and `export()` functions in Thomas Leeper's `rio` package.



# Looking forward

Make sure you are comfortable with piping, `dplyr`, and `tidyr` before beginning the [lesson on graphics](811/811-graphics), because we will use those concepts throughout. 

There are some other common data-munging tasks that we will put off until the final lesson:

- Writing your own functions
- Loops (and why you should not use them)
- `apply()` functions (and why you should use them instead of loops)
- Nesting and mapping, a tidy (and parallel!) method for applying complex functions across many datasets at once.



# Postscript on coding style

I would talk about coding style, but others can probably do that better than I can (though you can check the R scripts on Canvas for do-as-I-do examples). You can find lots of style guides for R online. They will broadly agree on how to write R with good style, but they won't agree on every fine point. Here are some that I endorse:

- a [short style guide](http://adv-r.had.co.nz/Style.html) by Hadley Wickham that will put you on the right track
- a [longer style guide](http://style.tidyverse.org/) (again by Wickham) that has general style guidance but also guidance specifically for working with the `tidyverse`, for those who want to be a little more obsessive about their programming style


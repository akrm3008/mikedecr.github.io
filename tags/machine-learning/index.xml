<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Michael DeCrescenzo</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Michael DeCrescenzo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael DeCrescenzo {year}</copyright>
    <lastBuildDate>Tue, 21 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fancy &#34;Neural Network&#34; Graphic but it&#39;s actually just Linear Regression
</title>
      <link>/post/linear-regression-neural-net/</link>
      <pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-regression-neural-net/</guid>
      <description>


&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;If you aren’t familiar with neural networks, you might have heard people (especially academics) joke about how “deep learning is just linear regression.”
This might sound hard to believe, because after all, neural networks sound fancy, and they often introduced with a graphic like this…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;net.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;which, despite being supposedly helpful, doesn’t really explain the intuition of what a neural network model really is.&lt;/p&gt;
&lt;p&gt;The purpose of this post is to demystify neural networks by taking something easy—linear regression—and drawing it up in a “neural network-style graphic.”
I very much intend to be cheeky about this: neural networks are not that complicated.
If you know the algebra behind a generalized linear model, you know how to understand neural networks.
The only reason this isn’t obvious is because neural networks have good marketing.
But that’s what this post takes aim at.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;glms-and-neural-network-jargon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GLMs and Neural Network Jargon&lt;/h2&gt;
&lt;p&gt;Neural network jargon contains terms like &lt;em&gt;inputs, outputs, hidden nodes, hidden layers, weights, biases, activation functions&lt;/em&gt;…each of which has a pretty easy mapping to the basic structure of a GLM.&lt;/p&gt;
&lt;p&gt;Let’s start with a linear model with Normal errors.
For units indexed &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the outcome &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; is a linear function of a vector of predictors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt; and coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.
We have &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; many predictors, so the length of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is also &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.
Finally, we have a random Normal error &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{i}\)&lt;/span&gt;…
&lt;span class=&#34;math display&#34; id=&#34;eq:ols&#34;&gt;\[\begin{align}
\begin{split}
  y_{i} &amp;amp;= \alpha + \mathbf{x}_{i}^{\intercal}\beta + \epsilon_{i} \\
  \epsilon_{i} &amp;amp;\sim \mathrm{Normal}\left(0, \sigma \right)
\end{split}  
\tag{1}
\end{align}\]&lt;/span&gt;
We could write this more generally by construing &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; itself as a random variable with a mean that is conditional on data and parameters…
&lt;span class=&#34;math display&#34; id=&#34;eq:glm&#34;&gt;\[\begin{align}
\begin{split}
  y_{i} &amp;amp;\sim 
    \mathrm{Normal}\left(\mu_{i}, \sigma \right) \\
    \mu_{i} &amp;amp;= \alpha + \mathbf{x}_{i}^{\intercal}\beta
\end{split}
\tag{2}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How does this relate to neural networks?
Already we have almost every element of the neural network jargon represented.
They go like so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;input&lt;/strong&gt;: predictors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;output&lt;/strong&gt;: dependent variable &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;weights&lt;/strong&gt;: a vector coefficients, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, length &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;biases&lt;/strong&gt;: a constant, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hidden node&lt;/strong&gt;: a linear combination of input data &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt;, weights &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and “bias term” &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. In regression this is equal to the linear predictor, &lt;span class=&#34;math inline&#34;&gt;\(\alpha + \mathbf{x}_{i}^{\intercal}\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;activation function&lt;/strong&gt;: a function the connects the linear predictor (or “hidden node”) to the dependent variable (“output”).
This is similar to an inverse link function, although the neural network is slightly more general.
In a linear regression example, the linear predictor is linked to the outcome by the identity function, so there is no substantive transformation of the linear predictor.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/post/linear-net/index_files/figure-html/net-reg-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://community.jmp.com/t5/JMP-Blog/Neural-networks-for-regression-lovers/ba-p/211796&#34; class=&#34;uri&#34;&gt;https://community.jmp.com/t5/JMP-Blog/Neural-networks-for-regression-lovers/ba-p/211796&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>

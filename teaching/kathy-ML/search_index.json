[
["index.html", "Machine Learning Methods for Automated Text and Content Analysis Introduction", " Machine Learning Methods for Automated Text and Content Analysis Michael G. DeCrescenzo Compiled September 07, 2020: 11:45 Introduction Primer on ML methods for text and content analysis. "],
["01-ML-stats.html", "Chapter 1 Statistics from the ML Point of View 1.1 Broad contours 1.2 Broad takeaways for academic statistics", " Chapter 1 Statistics from the ML Point of View “Machine learning” evolved somewhat separately in statistics as it did in computer science. As a result, there are important differences in philosophy and in jargon that are worth establishing from the outset. The purpose of this introduction is to lay out a broad enough view of the objectives of machine learning, discuss important differences in jargon, and draw a broad contrast between the predictive modeling workflow/philosophy and the “traditional academic statistics” workflow that we learn in political science. 1.1 Broad contours Personally I don’t so much like the phrase “machine learning” because it is hard to tell what counts as “statistics” and what counts as “machine learning.” My experience with the non-academic world suggests that all statistical methods belong to a class of methods known as “machine learning,” i.e. any method where the computer estimates patterns in data and has the capacity to predict new data. I personally like to use the phrase predictive models because it feels more closely aligned with what we’re actually doing, and more broadly encompassing predictive modeling approaches from both “statistics” and “computer science.” If I refer to “machine learning” or “ML,” I usually mean to distinguish the computer science methods from the more familiar statistical methods that we’re used to. Predictive models generally take the following form. We have some \\(y_{i}\\), an outcome measure for some unit \\(i\\), that we want to predict using \\(\\mathbf{x}_{i}\\), a vector of covariates for unit \\(i\\). An equation for \\(y_{i}\\) is given by \\[\\begin{align} y_{i} &amp;= f\\left(\\mathbf{x}_{i}\\right) + \\epsilon_{i} \\tag{1.1} \\end{align}\\] where \\(\\epsilon_{i}\\) is an error term. We’re remaining more general than any regression setup that we’re familiar with in political science or social science more broadly. All we have is \\(y_{i}\\) as a function of \\(\\mathbf{x}_{i}\\) plus error. Currently we’re imposing no functional form assumptions on \\(f(\\cdot)\\) and no distributional assumptions on \\(\\epsilon_{i}\\). Predictive models/“machine learning” is nothing but a bunch of models that people design to estimate \\(f(\\cdot)\\). Let \\(\\hat{f}(\\cdot)\\) be our predictive model for \\(f(\\cdot)\\), which lets usrelate the observed \\(y_{i}\\) to our model of choice. \\[\\begin{align} y_{i} &amp;= \\hat{f}\\left(\\mathbf{x}_{i}\\right) + e_{i} \\tag{1.2} \\end{align}\\] where \\(e_{i}\\) now represents the error in our model predictions. \\[\\begin{align} e_{i} &amp;= y_{i} - \\hat{f}\\left(\\mathbf{x}_{i}\\right) \\tag{1.3} \\end{align}\\] How do we pick \\(\\hat{f}(\\cdot)\\)? Sometimes by minimizing the error \\(\\epsilon_{i}\\), like simple OLS without distributional assumptions for the error. Sometimes we do make statistical assumptions about \\(\\epsilon_{i}\\) and then optimize a function of \\(\\epsilon_{i}\\) subject to the constrains of those assumptions, like with maximum likelihood. Because predictive methods are also used in industry settings, some methods work by assigning dollar values to the \\(\\epsilon_{i}\\) and optimizing a dollar-weighted error. This latter estimation objective introduces the notion of “loss,” which you can think about as a generalization of the notion of error. “Loss” is essentially a cost function of model inaccuracy: inaccurate predictions are mad, they lose money, and so the preferred model is the one that minimizes money lost. As a result, optimizing model accuracy on its own won’t be the best way to diagnose diseases if misdiagnosis in one direction is more costly than in the other direction. 1.1.1 Mapping between jargons Bold is ML jargon, and the rest is translation. Target. The dependent variable. The thing we’re trying to predict. Label. the dependent variable value. If I have “labeled data,” it means I know the target value for those observations. These are the observations often used to “train” a model especially in text settings. Features. Independent variables/covariates. Characteristics of units that inform us about the “target.” Objective function. The function we want to optimize in order to fit a model or make predictions. This could be residual sum of squares, a likelihood function, a posterior density, dollar-weighted combinations of any of these… Loss function. An objective function construed in terms of “loss.” For objective functions that we want to maximize (likelihood functions, posterior densities…) the loss function can just be a sign-flipped version of the objective function. Class. “Levels” of a discrete outcome variable. As in, we have an outcome variable with \\(K\\) possible classes, each indexed \\(k\\). For a binary outcome, \\(K = 2\\) and \\(k \\in \\{1, 2\\}\\). More generally, \\(k \\in \\{1, 2, \\ldots, K \\}\\). Classification. Predicting a class for a unit. Ordinarily we estimate probabilities in social science, but classification goes one step farther to make a decision about predicted “class membership” for each unit. This extra step usually requires a decision rule that maps predicted probabilities to predicted classes. ML theory considers optimal models for generating predicted probabilities as well as optimal decision rules (optimal in terms of loss). For instance, logit or multinomial logit estimate the probabilities of class membership, but K-nearest-neighbor classification simply says that the predicted class for unit \\(i\\) is the modal class among the neighboring observations, which entails a decision rule. Inference. This one bugs me to no end. In statistics, “inference” refers to decision-making about estimates under uncertainty, i.e. what are the statistical properties of my estimator and how do I leverage those properties to make decisions about my model. In machine learning, inference usually just means “prediction.” Once I fit a model, making a prediction for a new data point is “inference” about \\(y_i\\) given \\(\\mathbf{x}_{i}\\). This makes a little more sense if you mentally imagine a Bayesian analogy. I have updated beliefs about \\(y_{i} \\mid \\mathbf{x}_{i}\\), so prediction is like “posterior inference.” But, unaccountably, ML uses this language with no homage to Bayesian inference in particular, and in general any statistical consciousness at all is not strictly necessary. Training. Model fitting. “Training a model” means fitting it to data. The significance of “training” as opposed to “fitting” is usually that we have no interest in making predictions for model used to “fit” the data, so the quality of model “fit” for in-sample data is not an ingredient for marginal decision-making most of the time. Instead, the whole point of “training” is for the model to make predictions elsewhere. Learner. Infuriatingly, this just means “model.” Sometimes the “learner” jargin comes in handy for some models that are actually compositions/ensembles of other models, or compositions of many low-weight predictions. For instance, tree models (BART, random forests, etc.) generate a prediction as the sum or average of all of the predictions from many decision trees. Each individual tree is sometimes called a “weak learner,” since an individual tree isn’t very good, but aggregating over trees is better. Training set/training data. The data used for fitting a model. Test data/test set. Held-out data for measurement a model’s out-of-sample predictive performance. Typically the researcher considers many model configurations, and the chosen model configuration is the one that minimizes out-of-sample predictive loss (or maximizes out-of-sample “value” however measures). You have to give props to ML over academic stats here because the predictive performance of the model actually matters for decision-making, unlike in traditional academic stats where performance is almost never a concern, or if it is, it is measured in-sample and thus is prone to overfitting. Supervised learning. Fitting a model to data with known \\(y\\) and known \\(\\mathbf{x}\\). “Supervised” only means that for the data being used to fit the model, we know the correct left-hand data. Unsupervised learning. Looking for patterns where there is no perfect analogy to \\(y\\). This most typically includes dimensional reduction, where I have data of some unknown high dimensionality that I’m trying to simplify into fewer dimensions. The reduced space could be made of discrete clusters of observations, low-dimensional summaries of high dimensional data (e.g. “principal components”, latent factors, etc.). My soap-box is that this isn’t “unsupervised” at all, it’s just a model for a process that makes \\(\\mathbf{x}\\) instead of modeling the process that makes \\(y\\). Any model of latent structure probably fits in here, especially models where we have to discover the latent structure. In PS context this includes ideal point models, models for “democracy as a latent variable.” It is also where we will probably spend most of our time when it comes to topic models (cluster assignments for text), sentiment analysis (factor analysis for text), since the text itself is being modeled for its underlying structure. 1.1.2 Predictive accuracy/error Because most ML methods make predictive accuracy a preeminent goal, being explicit about the measure of predictive accuracy is essential. In a standard continuous outcome setting, this measure is often mean square error, \\[\\begin{align} \\mathit{MSE} &amp;= \\frac{1}{n} \\left(y_{i} - \\hat{y}_{i} \\right)^{2} \\tag{1.4} \\end{align}\\] where \\(\\hat{y}_{i} = \\hat{f}\\left(\\mathbf{x}_{i}\\right)\\). Sometimes this is discussed in terms of root mean square error, \\(\\mathit{RMSE} = \\sqrt{\\mathit{MSE}}\\). Reminder, MSE can be decomposed as the sum of of bias and variance. As long as either term is nonzero, MSE is nonzero: \\[\\begin{align} \\mathit{MSE} &amp;= \\mathit{Bias}\\left(\\hat{y}_{i}\\right)^2 + \\mathit{Var}\\left(\\hat{y}_{i}\\right) \\tag{1.5} \\end{align}\\] where \\(\\mathit{Bias}(y_{i}) = E\\left[y_{i} - E\\left[\\hat{y}_{i}\\right]\\right]\\). This is helpful to remember when people explain the benefits of one model over another in terms of which model settings move us which direction along the bias–variance trade-off. 1.1.3 Overfitting As stated before, ML methods consist of any method for functionally predicting \\(y\\) using \\(\\mathit{x}\\), and the predictive accuracy of that function is crucial. This is different from standard academic approaches, where our most important modeling goal1 is to interpret the prediction function even if other prediction functions out there would give better predictions. We might say that the standard social science workflow does not usually have much incentive to improve predictive accuracy (for most problems, measurement being an important exception). A natural consequence is that the typical modeling workflow does not contain tools or routines for controlling overfitting as a standard matter of course. In the ML workflow, however, it is impossible to conceive of model-building without dedicating major effort to preventing overfitting, because the primary decision margin for evaluating a model is its predictive performance out of sample, i.e. on data that the model has not seen before. All the same, the “social science statistical workflow” is at least aware of the potential problems with prediction. If we include too many meaningless predictors in a regression, we will improve the predictive accuracy of the model (e.g. increasing \\(R^{2}\\)) by fitting noise instead of actually-existing predictive signals in the data. The sections that follow discuss how to prevent overfitting in a general ML workflow. How these routines get implemented will naturally vary by the model type. 1.1.4 Sample splitting A crucial distinction in ML is the “training” vs. “test” data. AKA in-sample data vs. out-of-sample data. The main idea is that a complex model with lots of predictive flexibility can do a great job fitting in-sample data. More and more complex models can only ever do better predicting in-sample data. But that doesn’t make a model good. Models are well designed when they do a good job predicting out-of-sample data, or data that the model has never seen before. The ML workflow traditionally proceeds by taking any dataset and splitting it into a “training set” and “test set.” We fit a model on the training set, and generate predictions for the test set. As expected, our MSE will be higher in the training set, since those observations were used to fit the model, and MSE will be lower in the test set. We know that we are designing the best model not when its in-sample MSE is smallest, but when its out-of-sample MSE is smallest. Stated more succinctly, in the ML point of view, there is no point to a more complex model if it can’t predict unseen data better than a simpler model. The whole purpose for the model to exist is to predict unseen, out-of-sample data. 1.1.5 Regularization \\(\\DeclareMathOperator*{\\argmin}{arg\\,min}\\) How does a model increase out of sample performance? The main thing we want to do is regularize a model, or constrain its solutions to prevent overfitting. Standard models estimated in political science tend to be of an OLS or MLE framework. The OLS solution is \\[\\begin{align} \\hat{\\beta} &amp;= \\argmin_{\\beta} \\quad (\\mathbf{y} - X\\beta)^{\\intercal}(\\mathbf{y} - X\\beta) \\tag{1.6} \\end{align}\\] or, “find the \\(\\beta\\) values that would minimize the residual sum of squares.” This solution is unbiased (given the true model), but for overly complex models, predictions for new data are likely to have higher MSE than a model whose estimation is penalized in some way. Regularization is any method that penalizes the algorithm that fits a model. In the OLS case, one example is a “ridge regression estimator,” which penalizes the optimization problem by a factor of the overall coefficient magnitude. \\[\\begin{align} \\hat{\\beta} &amp;= \\argmin_{\\beta} \\quad (\\mathbf{y} - X\\beta)^{\\intercal}(\\mathbf{y} - X\\beta) + \\lambda \\sum_p \\beta_p \\tag{1.7} \\end{align}\\] where \\(\\lambda\\) is a penalty parameter that controls the amount of regularization. At \\(\\lambda = 0\\), the optimization problem simply is OLS. For larger \\(\\lambda\\) values, coefficients are shrunk toward zero, which introduces bias but tends to make more accurate predictions out-of-sample. As a religious Bayesian, I like to think about regularization as different methods for placing priors on model parameters. In fact, the ridge penalty is actually equivalent to giving the coefficients a Normal prior. This is an important connection to make because text models often use Bayesian estimation approaches due to the large number of parameters in a given model. Other models regularize in different ways. Tree models are biased toward “shorter trees.” Neural networks may “drop out” entire “nodes” to “simplify the signal” that passes through the network (zeroing out coefficients in a chain of regressions). 1.1.6 Tuning a model with cross-validation Once we introduce regularization into a model fitting/training problem, the researcher faces a choice of how much to regularize. For the ridge example, it isn’t obvious which value of \\(\\lambda\\) should be chosen (analogously, how tight the Normal prior should be on the coefficients). For tree models, we have choices to make about how short to keep the trees. These knobs are usually known as “hyperparameters” (parameters that control parameter estimation) or in a Bayesian context, \\[\\begin{align} \\beta &amp;\\sim \\mathrm{Normal}(0, \\gamma) \\end{align}\\] the hyperparameter \\(\\gamma\\) controls the amount of regularization. How do we choose which hyperparameters are best to prevent overfitting? The most common way to do this is to use cross-validation, which is an iterative sample splitting technique. Just as we split the sample above to train and test a model, cross-validation proceeds by chopping the data into many smaller pieces, testing hyperparameter values in each slice, and seeing which hyperparameters lead to optimal out-of-sample prediction. The way this usually goes is that there is an “initial split” to divide data into training and testing sets. After the initial split, the training data is divided into \\(K\\) many “folds,” where a model with a certain hyperparameter configuration is trained on \\(K-1\\) folds and then tested on fold \\(K\\). The \\(K\\) folds are used to select hyperparameter settings, and then the final test set is used to test the model that is most preferred by the cross validation routine. 1.1.7 Workflow Recap The model-training workflow, as a result, usually follows a pattern like initial split cross-validation of hyperparameters in training set choose best model from cross-validation test CV-chosen model on out-of-sample data repeat for other settings Typically you want to optimize the predictive error in step 4. 1.2 Broad takeaways for academic statistics Predictive modeling contrasts with the traditional academic workflow, even though it is useful for academics. In “academic stats,” we fit a model to all data and then make in-sample predictions. For flexible predictive models, this is a huge no-no. Even in academic stats, we try to generalize from data to something not in the data. So I think it does make sense to take a predictive modeling approach and develop a model for optimal out-of-sample prediction, since whatever “general reality” we’re trying to describe is not fully contained in the sample (most of the time). I think academics actually would agree with that, but they don’t typically do any of the model validation work. Why point this out? Because text papers WILL include model validation descriptions, and this will make it easier to understand why. Your methods classes probably described OLS and other estimates in terms of the “true model.” In the industry predictive modeling world, there isn’t usually any such thing. In the academic predictive modeling world, there is probably some hybrid that different papers will try to nail, but I don’t know if there’s any way to describe this trade-off perfectly. After getting those sweet, sweet significance stars and getting published.↩ "],
["02-papers.html", "Chapter 2 Papers 2.1 Shugars, “The Structure of Reasoning: Measuring Justification and Preferences in Text”", " Chapter 2 Papers 2.1 Shugars, “The Structure of Reasoning: Measuring Justification and Preferences in Text” This paper tries to build theoretical and operational models of political reasoning that takes place before opinions are expressed. If \\(y_{i}\\) is a survey response for person \\(i\\), what is the interactive network structure among their thoughts and ideas contained in \\(x_{i}\\)? Theoretically, we invoke a network model of political ideas and justifications. Some ideas are connected to other ideas, and that’s how conversations work. Psych, linguistic, and philosophical takes use networks to represent memories, argumentative premises, and normative ideas like “coherence” or moral underpinnings. Method has two main parts: Word embeddings. Use Google News corpus to create a “grammatical parse” or each word in grammatical structure. Create a network of original datasets out of the grammatical structures of each words. These networks are the basis of the original data analysis. 2.1.1 Word embedding A word embedding is a word’s location in a 300-D space. Call this a word’s “vector representation.” \\[\\begin{align} \\frac{1}{T} \\sum\\limits_{t = 1}^{T} \\sum\\limits_{-c \\leq j \\leq c: j \\neq 0} \\log p\\left(w_{t+j} \\mid w_{t} \\right) \\end{align}\\] For a sequence of training words \\(w_{1}, w_{2}, \\ldots, w_{T}\\) and a context window \\(c\\). In other words, a training model tries to predict word \\(w_{t + j}\\) using word \\(w_{t}\\), and the word vectors that are selected are the vectors that best predict each word’s surrounding words. This is used to define which words belong to which concepts. “In this paper, clusters of words are taken to refer to the same concept if all words in that cluster have cosine similarity greater than 0.5.” Concepts serve then as nodes (I think). 2.1.2 Network structure Use a “grammatical parse” to determine how nodes are connected, not just their proximity. Grammatical connection between nodes creates edges. All occurrences of a concept (read: word within a concept) are treated as a single node, so one node’s edges contain all grammatical connections between a concept and other concepts. 2.1.3 Analysis of similar networks You can construct a graph for each individual, but there is no guarantee that they contain overlapping nodes to compare structure. To compare two arbitrary networks, we use “portrait divergence” (Bagrow and Bollt 2019). Each graph (for each individual) has a “portrait” \\(B\\), which is an asymmetric matrix. Entry \\(B_{kl}\\) “captures the number of nodes \\(k\\) which have path length \\(l\\)”. From Bagrow and Bollt: \\(B_{kl}\\) is the number of nodes who have \\(k\\) nodes at distance \\(l\\). Similarity between two networks is measured as Kolmogoros-Smirnov statistic, which is the “maximum distance” between two networks. 2.1.4 Analysis of dissimilar networks: There is a suite of network connectivity measures (average degree, clustering, giant component percent, density) and network heterogeneity measures (standard deviation of degree, entropy, assortativity). There is a helpful Table 1 that describes these. 2.1.5 Original data An MTurk survey of free-response answers, and a secondary YouGov survey by Dan Hopkins and Hans Noel of ideological “Turing test” responses. The ideological Turing test asks participants to argue both sides of an issue, and see if they can do it convincingly. Only half of participants were sincere in their participation. Shugars interpretation is: only half are argumentatively structured so as to be meaningful, which we can take advantage of for analyzing the structure of reasoning. MTurk data: network measures are correlated with personality traits and ideology. That is, not just policy preferences, but the way those preferences are reasoned about! “…combination of network statistics suggests that progressive subjects tend to form networks with a core–periphery structure—that is, networks with an interconnected core of central ideas surrounded by a periphery of loosely connected auxiliary ideas.” And “Conservative subjects…produce more homogeneous networks in which each idea is roughly similarly connected, but further suggests these subjects tend to produce less content overall…We also see through the giant component metric that conservative subjects are more likely to produce networks with multiple, disconnected components while progressives are more like to produce connected networks, suggesting that a major difference in structure may be a tendency to ‘bridge’ between different clusters of distinct thought, with progressives more likely to tie disparate concepts together and conservatives more likely to articulate differing strains of though separately.” YouGov data: network stats predict the human-coded Turing test classification, meaning they capture the quality of reasoning. Shugars additionally asks if responses are driven by content similarity, or individual traits? Meaning, is \\(i\\)’s conservative response more similar to \\(i\\)’s liberal response (individual traits drive response similarity), or more similar to \\(j \\neq i\\)’s conservative response (content of response drives argumentative similarity)? Answer: individual is closer to themselves, suggesting that network structure captures individual level patterns in reasoning and thinking, not content-driven connections. "]
]

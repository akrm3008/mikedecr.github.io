---
title: "Experimentalists Agree! When Flat Priors Lead to Worse Learning"
author: Mike DeCrescenzo
description: "A Quick Comment on Gerber, Green, and Kaplan (2002)"
date: '2019-02-23'
slug: ggk-flat-priors
categories: ["Methods"]
tags: ["Experiments", "Bayesian statistics"]
comments: no
showcomments: yes
showpagemeta: yes
draft: true
# nature:
#       beforeInit: "https://platform.twitter.com/widgets.js"
---


I've been reading about Bayesian causal inference for a paper I'm hoping to write, and this has led me to dig into the work by [Gerber, Green, and Kaplan](https://books.google.com/books?hl=en&lr=&id=RDueAgAAQBAJ&oi=fnd&pg=PA9&dq=gerber+green+kaplan&ots=0I3f6Jfn33&sig=wVKOfRF39mkiT4vnMe8YecMMpeY#v=onepage&q=gerber%20green%20kaplan&f=false) about the "Illusion of Learning from Observational Research." In it, they put forth a model to describe how much you "update" your information about causal effects from experimental vs observational research.

The intuition of the model is to suppose that we want to learn about some causal effect $M$. When we conduct a study, our estimate $\hat{M}$ reflects the true effect $M$, plus bias $b$, plus error $u$:
\begin{align*}
  \hat{M} &= M + b + u,
\end{align*}

The authors show that if we want to update our priors about $M$, the amount of updating we do from the study depends on our priors about the degree of bias. If our study is experimental, where we can assume *a priori* that the bias $b$ is either zero or very small, our posterior uncertainty about the causal effect is greatly reduced, and we learn a lot about $M$. When we are in an observational study and have more diffuse priors about the size of the uncorrected bias $b$ (and the data don't allow us any way to update our prior about $b$) then we don't know exactly what we've learned about $M$ having looked at the data.

I think the intuition of this model is very good. It makes perfect sense that when you are less certain about the biases in a study, you are less certain about its findings. Fair!

# But

The namesake of the theoretical endeavor, the **Illusion of Learning from Observational Research**, comes from a result where we have *flat priors* about the bias $b$ and thus we learn *nothing* from having conducted an observational study.

What I want to assert is that this is a degenerate case that does not accurately characterize observational research and never really occurs in real life. 

Suppose we conduct an observational study and detect an effect of size $1.0$. If we are concerned about uncorrected bias in the study design, practically speaking we are mainly concerned with the possibility that the effect is over-estimated.^[
  Critics may be concerned that an effect is under-estimated but the prevailing emphasis on null-hypothesis testing strongly redirects critical attention away from this possibility.
] 
Chances are we think that the true effect is between 0 and the uncorrected observational effect, so we actually have pretty specific priors about the bias! Do we put much probability on the possibility that the true effect is *double* the estimated effect, or greater? Probably not, since we tend to worry that observational findings are primarily inflated by unobserved confounders and biased self-selection into treatment (though it is not impossible that the bias attenuates the estimate). Do we think the true effect is just as big as the estimated effect but in the exact opposite direction? Again, probably not, if careful theorizing and hypothesizing are underlying our expectations for the study in the first place.

Rearranging terms, your prior about the size of $b$ is the distribution of differences between your priors for the true and estimated effects. Setting $u$ to $0$ for a moment...
\begin{align*}
  b &= \hat{M} - M
\end{align*}
If you have some expectation about the true and estimated effects, you actually have reasonably clear priors about the size of the bias in observational studies. So the **Illusion** of learning from observational studies is a bit of an overstatement.

Confronting the fact that our priors are never flat, I low-key worry how many people have read the original piece and come away with the impression that flat priors about $b$ is a fair or accurate representation of observational research. I don't mean this as a defense of biased research, so much as a criticism of flat priors. I don't know if Gerber et al. intended for the flat priors case to be interpreted as "realistic"---on the one hand, the flat-priors result is the namesake of the piece, but their numerical example uses a non-flat prior for the bias term---but if a reader isn't already thinking hard about their priors, then it's easy to see how they might not catch this.


# Zooming out

There are a few mid-level lessons we could reinforce by thinking about our priors about bias in observational studies. 

First, there's never a bad time to remember that (improper) flat priors are unrealistic. If data are strong enough, obviously, inferences given flat priors can look like inferences given informed priors, but we should worry about any exercise where some theoretical result *necessarily depends* on an assumption of flat priors. In thought experiments and in real data analysis, you can always do better than a flat prior.

Relatedly, Bayesians are keen to highlight areas where informed priors provide important stability to some result that would have looked like nonsense under flat priors. This is one of those cases. Flat priors lead you in an unstable direction assessing the information conveyed by research. It's only in a case where you have more informed priors about the terms in the Gerber et al. model where the results conform to how we actually think about research findings.

Lastly, I am increasingly preoccupied by the way Bayes' theorem is routinely used in theoretical models to convey important intuitions about causal inference and yet there is nearly zero formal incorporation of Bayesian priors in applied data analysis of causal inference designs (in political science at least; other fields mix these things much more). I'm trying to write a paper about doing applied Bayesian analysis in causal inference, and I hope the causal inference crowd can be convinced to legalize it!


